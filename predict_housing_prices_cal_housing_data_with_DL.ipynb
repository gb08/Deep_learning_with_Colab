{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_housing_prices_cal_housing_data_with_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gb08/Deep_learning_with_Colab/blob/master/predict_housing_prices_cal_housing_data_with_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qWWwxddHpMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki2hyfImHqfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras  # tf.keras\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-7U22feSG-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynLLntBMTYW-",
        "colab_type": "text"
      },
      "source": [
        "Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3smjn6BEHqsi",
        "colab_type": "code",
        "outputId": "b277414d-a0a6-4c7a-a080-41ab88021748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "#!ls gdrive/My\\ Drive\n",
        "\n",
        "model_path = '/content/gdrive/yourpath'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzyzgQoIIWKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odVEJDZZI-S0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uTlbXhPTrF7",
        "colab_type": "text"
      },
      "source": [
        "**Download California Housing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ7T5xKSJabP",
        "colab_type": "code",
        "outputId": "7e2af7b4-7ba1-4948-f6c1-54b9c8e63dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fetch_california_housing = fetch_california_housing()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyG1gAUkKLgt",
        "colab_type": "code",
        "outputId": "d7da430a-b0f7-438a-87c0-3c752bd42127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "fetch_california_housing.DESCR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block\\n        - HouseAge      median house age in block\\n        - AveRooms      average number of rooms\\n        - AveBedrms     average number of bedrooms\\n        - Population    block population\\n        - AveOccup      average house occupancy\\n        - Latitude      house block latitude\\n        - Longitude     house block longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttp://lib.stat.cmu.edu/datasets/\\n\\nThe target variable is the median house value for California districts.\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4OG_rgXTuEy",
        "colab_type": "text"
      },
      "source": [
        "Get input features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1O_oshPKYQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = fetch_california_housing.data\n",
        "Y = fetch_california_housing.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWj3cMIhT5ix",
        "colab_type": "text"
      },
      "source": [
        "Split Training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2dVa3IgKyAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPV3CunT_5X",
        "colab_type": "text"
      },
      "source": [
        "Devide training set into training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvCyPGKGS_uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaiQYSL2TR5z",
        "colab_type": "code",
        "outputId": "3482c81a-bbcb-4d78-feb5-25fb1d998506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape, X_train.shape, X_val.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20640, 8), (9907, 8), (6605, 8), (4128, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERe4QqzUTgMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U9o3jNnUKCi",
        "colab_type": "text"
      },
      "source": [
        "**Standerdize input features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skkXzuGjTrw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_scaled = scaler.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggTZKNtUW2q",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc1lf5K9T1sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi5KWTrcUpSt",
        "colab_type": "code",
        "outputId": "06b1c968-97d1-4063-8eab-9546e1515e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test_scaled.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEs2YSgqUaGN",
        "colab_type": "text"
      },
      "source": [
        "**Initialize Keras Sequential model with One hidden layer with \"relu activation\", Optimizer as SGD with learning rate of (1e-3) and loss fnc \"mae\" **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXOVAjZAW8sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(30, input_shape=(8,)))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.SGD(1e-3), loss=\"mean_absolute_error\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck89wiWoU8ID",
        "colab_type": "text"
      },
      "source": [
        "Fit model on train set, Provide validation/holdout set. Store history object returned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUvSvAUYZnv8",
        "colab_type": "code",
        "outputId": "3a0e2098-528a-426b-8a62-36dede28d090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "callbacks = [keras.callbacks.EarlyStopping(patience=10)]\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    validation_data=(X_val_scaled, y_val), epochs=100,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 68us/sample - loss: 1.5009 - val_loss: 1.1875\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.9009 - val_loss: 0.8657\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.7120 - val_loss: 0.7461\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.6395 - val_loss: 0.6775\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5990 - val_loss: 0.6308\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.5726 - val_loss: 0.5960\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.5534 - val_loss: 0.5719\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5383 - val_loss: 0.5527\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.5255 - val_loss: 0.5374\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.5144 - val_loss: 0.5247\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.5046 - val_loss: 0.5146\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4960 - val_loss: 0.5062\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4886 - val_loss: 0.4996\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4819 - val_loss: 0.4942\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4762 - val_loss: 0.4902\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4710 - val_loss: 0.4858\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4665 - val_loss: 0.4829\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4627 - val_loss: 0.4798\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4591 - val_loss: 0.4767\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4558 - val_loss: 0.4743\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4532 - val_loss: 0.4722\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4506 - val_loss: 0.4697\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4483 - val_loss: 0.4678\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4462 - val_loss: 0.4662\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4442 - val_loss: 0.4646\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4424 - val_loss: 0.4633\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4408 - val_loss: 0.4612\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4393 - val_loss: 0.4603\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4379 - val_loss: 0.4581\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4366 - val_loss: 0.4571\n",
            "Epoch 31/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.4354 - val_loss: 0.4561\n",
            "Epoch 32/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4342 - val_loss: 0.4540\n",
            "Epoch 33/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4331 - val_loss: 0.4530\n",
            "Epoch 34/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4321 - val_loss: 0.4514\n",
            "Epoch 35/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4311 - val_loss: 0.4510\n",
            "Epoch 36/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.4301 - val_loss: 0.4495\n",
            "Epoch 37/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4292 - val_loss: 0.4488\n",
            "Epoch 38/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4285 - val_loss: 0.4480\n",
            "Epoch 39/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4275 - val_loss: 0.4473\n",
            "Epoch 40/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4268 - val_loss: 0.4459\n",
            "Epoch 41/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4258 - val_loss: 0.4442\n",
            "Epoch 42/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4252 - val_loss: 0.4421\n",
            "Epoch 43/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4244 - val_loss: 0.4421\n",
            "Epoch 44/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4237 - val_loss: 0.4420\n",
            "Epoch 45/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4229 - val_loss: 0.4405\n",
            "Epoch 46/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4221 - val_loss: 0.4389\n",
            "Epoch 47/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4214 - val_loss: 0.4380\n",
            "Epoch 48/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4208 - val_loss: 0.4365\n",
            "Epoch 49/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4201 - val_loss: 0.4358\n",
            "Epoch 50/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4195 - val_loss: 0.4346\n",
            "Epoch 51/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4190 - val_loss: 0.4334\n",
            "Epoch 52/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4180 - val_loss: 0.4338\n",
            "Epoch 53/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4175 - val_loss: 0.4310\n",
            "Epoch 54/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4170 - val_loss: 0.4301\n",
            "Epoch 55/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4163 - val_loss: 0.4293\n",
            "Epoch 56/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4157 - val_loss: 0.4281\n",
            "Epoch 57/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4151 - val_loss: 0.4280\n",
            "Epoch 58/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4145 - val_loss: 0.4264\n",
            "Epoch 59/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4140 - val_loss: 0.4259\n",
            "Epoch 60/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4134 - val_loss: 0.4258\n",
            "Epoch 61/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4128 - val_loss: 0.4248\n",
            "Epoch 62/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4123 - val_loss: 0.4236\n",
            "Epoch 63/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4118 - val_loss: 0.4221\n",
            "Epoch 64/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4113 - val_loss: 0.4222\n",
            "Epoch 65/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4108 - val_loss: 0.4213\n",
            "Epoch 66/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4104 - val_loss: 0.4209\n",
            "Epoch 67/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4099 - val_loss: 0.4206\n",
            "Epoch 68/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4095 - val_loss: 0.4198\n",
            "Epoch 69/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4090 - val_loss: 0.4203\n",
            "Epoch 70/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4087 - val_loss: 0.4177\n",
            "Epoch 71/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4082 - val_loss: 0.4171\n",
            "Epoch 72/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.4079 - val_loss: 0.4175\n",
            "Epoch 73/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4075 - val_loss: 0.4168\n",
            "Epoch 74/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4071 - val_loss: 0.4170\n",
            "Epoch 75/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4068 - val_loss: 0.4165\n",
            "Epoch 76/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4064 - val_loss: 0.4158\n",
            "Epoch 77/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4062 - val_loss: 0.4153\n",
            "Epoch 78/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4058 - val_loss: 0.4152\n",
            "Epoch 79/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4055 - val_loss: 0.4147\n",
            "Epoch 80/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4050 - val_loss: 0.4150\n",
            "Epoch 81/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4049 - val_loss: 0.4146\n",
            "Epoch 82/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4042 - val_loss: 0.4145\n",
            "Epoch 83/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4042 - val_loss: 0.4128\n",
            "Epoch 84/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4039 - val_loss: 0.4123\n",
            "Epoch 85/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4035 - val_loss: 0.4131\n",
            "Epoch 86/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4032 - val_loss: 0.4119\n",
            "Epoch 87/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4030 - val_loss: 0.4120\n",
            "Epoch 88/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4028 - val_loss: 0.4114\n",
            "Epoch 89/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4024 - val_loss: 0.4113\n",
            "Epoch 90/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4021 - val_loss: 0.4111\n",
            "Epoch 91/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4018 - val_loss: 0.4122\n",
            "Epoch 92/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4015 - val_loss: 0.4125\n",
            "Epoch 93/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4014 - val_loss: 0.4111\n",
            "Epoch 94/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4010 - val_loss: 0.4096\n",
            "Epoch 95/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4006 - val_loss: 0.4096\n",
            "Epoch 96/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4006 - val_loss: 0.4098\n",
            "Epoch 97/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4003 - val_loss: 0.4104\n",
            "Epoch 98/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4000 - val_loss: 0.4099\n",
            "Epoch 99/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3999 - val_loss: 0.4093\n",
            "Epoch 100/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3995 - val_loss: 0.4093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmd60d82Z2GT",
        "colab_type": "code",
        "outputId": "eac47036-8fdd-4967-957b-d92c87d0f176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4128/4128 [==============================] - 0s 23us/sample - loss: 0.4127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4127184997002284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upaooUmHZ613",
        "colab_type": "code",
        "outputId": "10d16251-464d-44ca-f59f-19397c33b073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.2893362],\n",
              "       [3.2388365],\n",
              "       [0.7518293],\n",
              "       ...,\n",
              "       [3.2417667],\n",
              "       [1.2216382],\n",
              "       [1.4829005]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NOyymxLVdfX",
        "colab_type": "text"
      },
      "source": [
        "Plot Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoHvb9ERZ99-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_learning_curves(history):\n",
        "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0, 1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aqQVggMZ95T",
        "colab_type": "code",
        "outputId": "9e3f2edd-9d22-4a85-c81b-dd86aeb46d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "plot_learning_curves(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZwpjERERnymMRUREfPb/Ab24j1svBYLrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMYnYs84VjCm",
        "colab_type": "text"
      },
      "source": [
        "Check model Performance with different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs7hGt_baLQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]\n",
        "histories = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmNZCMVFVsQf",
        "colab_type": "text"
      },
      "source": [
        "This is very basic method. Loop over learning rates. Compile and fit model for each. Store history for training details of each model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3FFOdSh_DF",
        "colab_type": "code",
        "outputId": "c35f2048-e7dd-46fe-bd54-9719731d11b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for learning_rate in learning_rates:\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
        "\n",
        "    callbacks = [keras.callbacks.EarlyStopping(patience=10)]\n",
        "\n",
        "    history = model.fit(X_train_scaled, y_train,\n",
        "                        validation_data=(X_val_scaled, y_val), epochs=100,\n",
        "                        callbacks=callbacks)\n",
        "    histories.append(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 67us/sample - loss: 5.4455 - val_loss: 6.7445\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 4.2195 - val_loss: 5.9144\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 3.3420 - val_loss: 5.2748\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 2.6954 - val_loss: 4.7752\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 2.2117 - val_loss: 4.3463\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 1.8460 - val_loss: 3.9570\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 1.5661 - val_loss: 3.5911\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 1.3516 - val_loss: 3.2455\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 1.1861 - val_loss: 2.9259\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 1.0578 - val_loss: 2.6354\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.9583 - val_loss: 2.3670\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.8810 - val_loss: 2.1243\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.8207 - val_loss: 1.9063\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.7734 - val_loss: 1.7143\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.7362 - val_loss: 1.5437\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.7066 - val_loss: 1.3949\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.6829 - val_loss: 1.2650\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.6638 - val_loss: 1.1530\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.6481 - val_loss: 1.0560\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.6352 - val_loss: 0.9719\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.6244 - val_loss: 0.9004\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.6152 - val_loss: 0.8427\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.6073 - val_loss: 0.7933\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.6005 - val_loss: 0.7510\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5944 - val_loss: 0.7145\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5889 - val_loss: 0.6830\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.5840 - val_loss: 0.6564\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5795 - val_loss: 0.6339\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5753 - val_loss: 0.6149\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5714 - val_loss: 0.6000\n",
            "Epoch 31/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5677 - val_loss: 0.5873\n",
            "Epoch 32/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5643 - val_loss: 0.5767\n",
            "Epoch 33/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.5610 - val_loss: 0.5676\n",
            "Epoch 34/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.5579 - val_loss: 0.5600\n",
            "Epoch 35/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5550 - val_loss: 0.5536\n",
            "Epoch 36/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5522 - val_loss: 0.5480\n",
            "Epoch 37/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5494 - val_loss: 0.5434\n",
            "Epoch 38/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5468 - val_loss: 0.5396\n",
            "Epoch 39/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.5443 - val_loss: 0.5363\n",
            "Epoch 40/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.5419 - val_loss: 0.5336\n",
            "Epoch 41/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5395 - val_loss: 0.5312\n",
            "Epoch 42/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5372 - val_loss: 0.5292\n",
            "Epoch 43/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5350 - val_loss: 0.5274\n",
            "Epoch 44/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5328 - val_loss: 0.5259\n",
            "Epoch 45/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5307 - val_loss: 0.5246\n",
            "Epoch 46/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5287 - val_loss: 0.5235\n",
            "Epoch 47/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.5267 - val_loss: 0.5225\n",
            "Epoch 48/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5247 - val_loss: 0.5216\n",
            "Epoch 49/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.5229 - val_loss: 0.5207\n",
            "Epoch 50/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.5210 - val_loss: 0.5199\n",
            "Epoch 51/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5192 - val_loss: 0.5192\n",
            "Epoch 52/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5175 - val_loss: 0.5185\n",
            "Epoch 53/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5157 - val_loss: 0.5177\n",
            "Epoch 54/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.5141 - val_loss: 0.5170\n",
            "Epoch 55/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.5124 - val_loss: 0.5163\n",
            "Epoch 56/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.5108 - val_loss: 0.5155\n",
            "Epoch 57/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.5092 - val_loss: 0.5147\n",
            "Epoch 58/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5077 - val_loss: 0.5138\n",
            "Epoch 59/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.5062 - val_loss: 0.5130\n",
            "Epoch 60/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5047 - val_loss: 0.5121\n",
            "Epoch 61/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.5033 - val_loss: 0.5111\n",
            "Epoch 62/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5019 - val_loss: 0.5102\n",
            "Epoch 63/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.5005 - val_loss: 0.5092\n",
            "Epoch 64/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4991 - val_loss: 0.5082\n",
            "Epoch 65/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4978 - val_loss: 0.5071\n",
            "Epoch 66/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4965 - val_loss: 0.5061\n",
            "Epoch 67/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4952 - val_loss: 0.5050\n",
            "Epoch 68/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4939 - val_loss: 0.5040\n",
            "Epoch 69/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4927 - val_loss: 0.5029\n",
            "Epoch 70/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4915 - val_loss: 0.5017\n",
            "Epoch 71/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4903 - val_loss: 0.5007\n",
            "Epoch 72/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4891 - val_loss: 0.4995\n",
            "Epoch 73/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4879 - val_loss: 0.4984\n",
            "Epoch 74/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4868 - val_loss: 0.4972\n",
            "Epoch 75/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4857 - val_loss: 0.4960\n",
            "Epoch 76/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4846 - val_loss: 0.4947\n",
            "Epoch 77/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4835 - val_loss: 0.4935\n",
            "Epoch 78/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4824 - val_loss: 0.4923\n",
            "Epoch 79/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4814 - val_loss: 0.4910\n",
            "Epoch 80/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4803 - val_loss: 0.4898\n",
            "Epoch 81/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4793 - val_loss: 0.4886\n",
            "Epoch 82/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4783 - val_loss: 0.4874\n",
            "Epoch 83/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4774 - val_loss: 0.4862\n",
            "Epoch 84/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4764 - val_loss: 0.4851\n",
            "Epoch 85/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4754 - val_loss: 0.4838\n",
            "Epoch 86/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.4745 - val_loss: 0.4826\n",
            "Epoch 87/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4736 - val_loss: 0.4814\n",
            "Epoch 88/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4727 - val_loss: 0.4802\n",
            "Epoch 89/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4718 - val_loss: 0.4791\n",
            "Epoch 90/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4709 - val_loss: 0.4779\n",
            "Epoch 91/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4700 - val_loss: 0.4767\n",
            "Epoch 92/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4692 - val_loss: 0.4756\n",
            "Epoch 93/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.4683 - val_loss: 0.4744\n",
            "Epoch 94/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4675 - val_loss: 0.4732\n",
            "Epoch 95/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4667 - val_loss: 0.4721\n",
            "Epoch 96/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4659 - val_loss: 0.4710\n",
            "Epoch 97/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4651 - val_loss: 0.4699\n",
            "Epoch 98/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4643 - val_loss: 0.4688\n",
            "Epoch 99/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4635 - val_loss: 0.4678\n",
            "Epoch 100/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4628 - val_loss: 0.4667\n",
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 73us/sample - loss: 4.8483 - val_loss: 2.8470\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 1.9173 - val_loss: 2.3127\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 1.1490 - val_loss: 2.1394\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.9179 - val_loss: 1.6289\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.8287 - val_loss: 1.1833\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.7821 - val_loss: 0.9432\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.7517 - val_loss: 0.8061\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.7289 - val_loss: 0.7343\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.7099 - val_loss: 0.6969\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.6933 - val_loss: 0.6793\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.6781 - val_loss: 0.6698\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.6640 - val_loss: 0.6636\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.6508 - val_loss: 0.6573\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.6383 - val_loss: 0.6494\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.6263 - val_loss: 0.6442\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.6150 - val_loss: 0.6337\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 66us/sample - loss: 0.6041 - val_loss: 0.6258\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.5937 - val_loss: 0.6210\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.5839 - val_loss: 0.6135\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 69us/sample - loss: 0.5745 - val_loss: 0.6036\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 64us/sample - loss: 0.5655 - val_loss: 0.5895\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.5569 - val_loss: 0.5792\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 65us/sample - loss: 0.5487 - val_loss: 0.5662\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.5409 - val_loss: 0.5598\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.5335 - val_loss: 0.5488\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5264 - val_loss: 0.5417\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.5197 - val_loss: 0.5345\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.5133 - val_loss: 0.5260\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.5073 - val_loss: 0.5195\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5016 - val_loss: 0.5090\n",
            "Epoch 31/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.4961 - val_loss: 0.5021\n",
            "Epoch 32/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4909 - val_loss: 0.4954\n",
            "Epoch 33/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4859 - val_loss: 0.4869\n",
            "Epoch 34/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4813 - val_loss: 0.4821\n",
            "Epoch 35/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4768 - val_loss: 0.4763\n",
            "Epoch 36/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4727 - val_loss: 0.4714\n",
            "Epoch 37/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4687 - val_loss: 0.4655\n",
            "Epoch 38/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4650 - val_loss: 0.4609\n",
            "Epoch 39/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4615 - val_loss: 0.4569\n",
            "Epoch 40/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4581 - val_loss: 0.4527\n",
            "Epoch 41/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4549 - val_loss: 0.4489\n",
            "Epoch 42/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4519 - val_loss: 0.4462\n",
            "Epoch 43/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4491 - val_loss: 0.4428\n",
            "Epoch 44/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4463 - val_loss: 0.4400\n",
            "Epoch 45/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4438 - val_loss: 0.4371\n",
            "Epoch 46/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4413 - val_loss: 0.4349\n",
            "Epoch 47/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4390 - val_loss: 0.4323\n",
            "Epoch 48/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4368 - val_loss: 0.4304\n",
            "Epoch 49/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4347 - val_loss: 0.4283\n",
            "Epoch 50/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4327 - val_loss: 0.4266\n",
            "Epoch 51/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4308 - val_loss: 0.4250\n",
            "Epoch 52/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4290 - val_loss: 0.4233\n",
            "Epoch 53/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4273 - val_loss: 0.4218\n",
            "Epoch 54/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4256 - val_loss: 0.4206\n",
            "Epoch 55/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4241 - val_loss: 0.4194\n",
            "Epoch 56/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4226 - val_loss: 0.4181\n",
            "Epoch 57/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4212 - val_loss: 0.4171\n",
            "Epoch 58/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.4198 - val_loss: 0.4160\n",
            "Epoch 59/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4185 - val_loss: 0.4150\n",
            "Epoch 60/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4173 - val_loss: 0.4141\n",
            "Epoch 61/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4161 - val_loss: 0.4133\n",
            "Epoch 62/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4150 - val_loss: 0.4122\n",
            "Epoch 63/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4139 - val_loss: 0.4111\n",
            "Epoch 64/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4128 - val_loss: 0.4102\n",
            "Epoch 65/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4118 - val_loss: 0.4093\n",
            "Epoch 66/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4108 - val_loss: 0.4085\n",
            "Epoch 67/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4099 - val_loss: 0.4078\n",
            "Epoch 68/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4090 - val_loss: 0.4072\n",
            "Epoch 69/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4081 - val_loss: 0.4063\n",
            "Epoch 70/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4072 - val_loss: 0.4061\n",
            "Epoch 71/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4064 - val_loss: 0.4054\n",
            "Epoch 72/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4056 - val_loss: 0.4049\n",
            "Epoch 73/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4048 - val_loss: 0.4045\n",
            "Epoch 74/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4041 - val_loss: 0.4039\n",
            "Epoch 75/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4034 - val_loss: 0.4029\n",
            "Epoch 76/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4027 - val_loss: 0.4022\n",
            "Epoch 77/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4020 - val_loss: 0.4023\n",
            "Epoch 78/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4013 - val_loss: 0.4017\n",
            "Epoch 79/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4007 - val_loss: 0.4008\n",
            "Epoch 80/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4001 - val_loss: 0.4002\n",
            "Epoch 81/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3995 - val_loss: 0.3994\n",
            "Epoch 82/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3989 - val_loss: 0.3995\n",
            "Epoch 83/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3984 - val_loss: 0.3989\n",
            "Epoch 84/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3979 - val_loss: 0.3984\n",
            "Epoch 85/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3973 - val_loss: 0.3979\n",
            "Epoch 86/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3968 - val_loss: 0.3977\n",
            "Epoch 87/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3963 - val_loss: 0.3970\n",
            "Epoch 88/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3958 - val_loss: 0.3963\n",
            "Epoch 89/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3954 - val_loss: 0.3952\n",
            "Epoch 90/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3949 - val_loss: 0.3950\n",
            "Epoch 91/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3945 - val_loss: 0.3947\n",
            "Epoch 92/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3940 - val_loss: 0.3948\n",
            "Epoch 93/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3936 - val_loss: 0.3942\n",
            "Epoch 94/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3932 - val_loss: 0.3937\n",
            "Epoch 95/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3928 - val_loss: 0.3931\n",
            "Epoch 96/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3924 - val_loss: 0.3929\n",
            "Epoch 97/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3920 - val_loss: 0.3925\n",
            "Epoch 98/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3916 - val_loss: 0.3920\n",
            "Epoch 99/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3912 - val_loss: 0.3918\n",
            "Epoch 100/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3908 - val_loss: 0.3917\n",
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 70us/sample - loss: 2.7255 - val_loss: 2.8403\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.9795 - val_loss: 0.9752\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.7428 - val_loss: 0.7173\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.6764 - val_loss: 0.6503\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.6364 - val_loss: 0.6187\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.6039 - val_loss: 0.5830\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5758 - val_loss: 0.5656\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.5512 - val_loss: 0.5413\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.5293 - val_loss: 0.5230\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.5104 - val_loss: 0.5010\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4937 - val_loss: 0.4892\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4793 - val_loss: 0.4794\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4669 - val_loss: 0.4643\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4557 - val_loss: 0.4585\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4464 - val_loss: 0.4512\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4383 - val_loss: 0.4368\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4311 - val_loss: 0.4389\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.4252 - val_loss: 0.4427\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4198 - val_loss: 0.4320\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.4153 - val_loss: 0.4289\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4113 - val_loss: 0.4193\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4077 - val_loss: 0.4186\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4044 - val_loss: 0.4087\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.4017 - val_loss: 0.4038\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3993 - val_loss: 0.4011\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3970 - val_loss: 0.4067\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3950 - val_loss: 0.4011\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3927 - val_loss: 0.4075\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3913 - val_loss: 0.4117\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3899 - val_loss: 0.3985\n",
            "Epoch 31/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3882 - val_loss: 0.3915\n",
            "Epoch 32/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3869 - val_loss: 0.3921\n",
            "Epoch 33/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3854 - val_loss: 0.3971\n",
            "Epoch 34/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3842 - val_loss: 0.3931\n",
            "Epoch 35/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3831 - val_loss: 0.3869\n",
            "Epoch 36/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3820 - val_loss: 0.3852\n",
            "Epoch 37/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3810 - val_loss: 0.3857\n",
            "Epoch 38/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3798 - val_loss: 0.3848\n",
            "Epoch 39/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3791 - val_loss: 0.3857\n",
            "Epoch 40/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3780 - val_loss: 0.3842\n",
            "Epoch 41/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3771 - val_loss: 0.3810\n",
            "Epoch 42/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3762 - val_loss: 0.3843\n",
            "Epoch 43/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3755 - val_loss: 0.3797\n",
            "Epoch 44/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3747 - val_loss: 0.3815\n",
            "Epoch 45/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3740 - val_loss: 0.3829\n",
            "Epoch 46/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3733 - val_loss: 0.3852\n",
            "Epoch 47/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3728 - val_loss: 0.3798\n",
            "Epoch 48/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3721 - val_loss: 0.3761\n",
            "Epoch 49/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3713 - val_loss: 0.3770\n",
            "Epoch 50/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3707 - val_loss: 0.3785\n",
            "Epoch 51/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3702 - val_loss: 0.3739\n",
            "Epoch 52/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3695 - val_loss: 0.3770\n",
            "Epoch 53/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3690 - val_loss: 0.3768\n",
            "Epoch 54/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3684 - val_loss: 0.3805\n",
            "Epoch 55/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3679 - val_loss: 0.3746\n",
            "Epoch 56/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3673 - val_loss: 0.3754\n",
            "Epoch 57/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3669 - val_loss: 0.3725\n",
            "Epoch 58/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3664 - val_loss: 0.3741\n",
            "Epoch 59/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3658 - val_loss: 0.3719\n",
            "Epoch 60/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3655 - val_loss: 0.3721\n",
            "Epoch 61/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3648 - val_loss: 0.3725\n",
            "Epoch 62/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3644 - val_loss: 0.3750\n",
            "Epoch 63/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3640 - val_loss: 0.3713\n",
            "Epoch 64/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3636 - val_loss: 0.3698\n",
            "Epoch 65/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3631 - val_loss: 0.3738\n",
            "Epoch 66/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3628 - val_loss: 0.3687\n",
            "Epoch 67/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3623 - val_loss: 0.3682\n",
            "Epoch 68/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3620 - val_loss: 0.3651\n",
            "Epoch 69/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3615 - val_loss: 0.3685\n",
            "Epoch 70/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3611 - val_loss: 0.3666\n",
            "Epoch 71/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3606 - val_loss: 0.3704\n",
            "Epoch 72/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3603 - val_loss: 0.3655\n",
            "Epoch 73/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3601 - val_loss: 0.3676\n",
            "Epoch 74/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3597 - val_loss: 0.3741\n",
            "Epoch 75/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3594 - val_loss: 0.3707\n",
            "Epoch 76/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3590 - val_loss: 0.3698\n",
            "Epoch 77/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3586 - val_loss: 0.3682\n",
            "Epoch 78/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3583 - val_loss: 0.3717\n",
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 75us/sample - loss: 1.2704 - val_loss: 0.6547\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.5711 - val_loss: 0.5315\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.5048 - val_loss: 0.4972\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.4668 - val_loss: 0.4627\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4421 - val_loss: 0.4677\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4263 - val_loss: 0.4872\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4150 - val_loss: 0.4366\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4069 - val_loss: 0.4201\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4008 - val_loss: 0.4130\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3962 - val_loss: 0.4275\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3916 - val_loss: 0.4355\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3884 - val_loss: 0.4304\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3850 - val_loss: 0.4104\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3816 - val_loss: 0.4688\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3796 - val_loss: 0.4290\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3768 - val_loss: 0.4417\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3745 - val_loss: 0.3968\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3717 - val_loss: 0.3874\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3704 - val_loss: 0.4180\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3686 - val_loss: 0.4228\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3666 - val_loss: 0.3911\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3651 - val_loss: 0.4275\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3643 - val_loss: 0.3776\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3625 - val_loss: 0.3722\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3613 - val_loss: 0.4312\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3600 - val_loss: 0.4213\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3590 - val_loss: 0.3738\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3580 - val_loss: 0.4222\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3569 - val_loss: 0.4328\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3560 - val_loss: 0.3942\n",
            "Epoch 31/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3552 - val_loss: 0.3988\n",
            "Epoch 32/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3540 - val_loss: 0.4059\n",
            "Epoch 33/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.3532 - val_loss: 0.3801\n",
            "Epoch 34/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3519 - val_loss: 0.3626\n",
            "Epoch 35/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3515 - val_loss: 0.3966\n",
            "Epoch 36/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3514 - val_loss: 0.3746\n",
            "Epoch 37/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3502 - val_loss: 0.3670\n",
            "Epoch 38/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3492 - val_loss: 0.3617\n",
            "Epoch 39/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3486 - val_loss: 0.3878\n",
            "Epoch 40/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3479 - val_loss: 0.3687\n",
            "Epoch 41/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3475 - val_loss: 0.3651\n",
            "Epoch 42/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3470 - val_loss: 0.3682\n",
            "Epoch 43/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3457 - val_loss: 0.3657\n",
            "Epoch 44/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3457 - val_loss: 0.3587\n",
            "Epoch 45/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3450 - val_loss: 0.3642\n",
            "Epoch 46/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3441 - val_loss: 0.4030\n",
            "Epoch 47/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3435 - val_loss: 0.3686\n",
            "Epoch 48/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3430 - val_loss: 0.3534\n",
            "Epoch 49/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3418 - val_loss: 0.3536\n",
            "Epoch 50/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3418 - val_loss: 0.3609\n",
            "Epoch 51/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3409 - val_loss: 0.3649\n",
            "Epoch 52/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3406 - val_loss: 0.3544\n",
            "Epoch 53/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3396 - val_loss: 0.3499\n",
            "Epoch 54/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3392 - val_loss: 0.3739\n",
            "Epoch 55/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3382 - val_loss: 0.3920\n",
            "Epoch 56/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3376 - val_loss: 0.3971\n",
            "Epoch 57/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3377 - val_loss: 0.3464\n",
            "Epoch 58/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3364 - val_loss: 0.3482\n",
            "Epoch 59/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3359 - val_loss: 0.3503\n",
            "Epoch 60/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3354 - val_loss: 0.3579\n",
            "Epoch 61/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3347 - val_loss: 0.3582\n",
            "Epoch 62/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3343 - val_loss: 0.3466\n",
            "Epoch 63/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3341 - val_loss: 0.3531\n",
            "Epoch 64/100\n",
            "9907/9907 [==============================] - 1s 51us/sample - loss: 0.3333 - val_loss: 0.3424\n",
            "Epoch 65/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3326 - val_loss: 0.3797\n",
            "Epoch 66/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3321 - val_loss: 0.3725\n",
            "Epoch 67/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3322 - val_loss: 0.3470\n",
            "Epoch 68/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3313 - val_loss: 0.3431\n",
            "Epoch 69/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3309 - val_loss: 0.3447\n",
            "Epoch 70/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3299 - val_loss: 0.3598\n",
            "Epoch 71/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3296 - val_loss: 0.3410\n",
            "Epoch 72/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3290 - val_loss: 0.3405\n",
            "Epoch 73/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3290 - val_loss: 0.3646\n",
            "Epoch 74/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3280 - val_loss: 0.3718\n",
            "Epoch 75/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3282 - val_loss: 0.3409\n",
            "Epoch 76/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3275 - val_loss: 0.3857\n",
            "Epoch 77/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3271 - val_loss: 0.3499\n",
            "Epoch 78/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3259 - val_loss: 0.3433\n",
            "Epoch 79/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3259 - val_loss: 0.3529\n",
            "Epoch 80/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3257 - val_loss: 0.3374\n",
            "Epoch 81/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3250 - val_loss: 0.3369\n",
            "Epoch 82/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3247 - val_loss: 0.3399\n",
            "Epoch 83/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3241 - val_loss: 0.3532\n",
            "Epoch 84/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3233 - val_loss: 0.3707\n",
            "Epoch 85/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3229 - val_loss: 0.3450\n",
            "Epoch 86/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3226 - val_loss: 0.3427\n",
            "Epoch 87/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3223 - val_loss: 0.3925\n",
            "Epoch 88/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3216 - val_loss: 0.4202\n",
            "Epoch 89/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3214 - val_loss: 0.3366\n",
            "Epoch 90/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3207 - val_loss: 0.4192\n",
            "Epoch 91/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3205 - val_loss: 0.3804\n",
            "Epoch 92/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3211 - val_loss: 0.3519\n",
            "Epoch 93/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3195 - val_loss: 0.3648\n",
            "Epoch 94/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3196 - val_loss: 0.3336\n",
            "Epoch 95/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3190 - val_loss: 0.3881\n",
            "Epoch 96/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3184 - val_loss: 0.3698\n",
            "Epoch 97/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3187 - val_loss: 0.3309\n",
            "Epoch 98/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3179 - val_loss: 0.3447\n",
            "Epoch 99/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3172 - val_loss: 0.3746\n",
            "Epoch 100/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3172 - val_loss: 0.3306\n",
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 85us/sample - loss: 0.7643 - val_loss: 0.6097\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4412 - val_loss: 0.5347\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.4069 - val_loss: 0.4294\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3896 - val_loss: 0.6746\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3809 - val_loss: 0.4948\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3746 - val_loss: 0.4111\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3697 - val_loss: 0.4135\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3642 - val_loss: 0.3972\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3623 - val_loss: 0.5124\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3589 - val_loss: 0.3727\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3563 - val_loss: 0.3886\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3532 - val_loss: 0.4543\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3514 - val_loss: 0.4521\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3476 - val_loss: 0.4431\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3470 - val_loss: 0.3518\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 52us/sample - loss: 0.3453 - val_loss: 0.3722\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3418 - val_loss: 0.3552\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3414 - val_loss: 0.3445\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3413 - val_loss: 0.6996\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3401 - val_loss: 0.3407\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3357 - val_loss: 0.3476\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3350 - val_loss: 0.4076\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3328 - val_loss: 0.5471\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3331 - val_loss: 0.4412\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3314 - val_loss: 1.9017\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3501 - val_loss: 0.4691\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3297 - val_loss: 2.8235\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3304 - val_loss: 0.6223\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3285 - val_loss: 0.3582\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3259 - val_loss: 0.6667\n",
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 83us/sample - loss: 0.5878 - val_loss: 4.6647\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.5069 - val_loss: 0.6660\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.4020 - val_loss: 0.4065\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3943 - val_loss: 0.3846\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3782 - val_loss: 0.4050\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3753 - val_loss: 0.3852\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3678 - val_loss: 0.4774\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3720 - val_loss: 0.3716\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3675 - val_loss: 0.3947\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3628 - val_loss: 0.3609\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3612 - val_loss: 0.4058\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3681 - val_loss: 0.3810\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3571 - val_loss: 0.3962\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3540 - val_loss: 0.4622\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3515 - val_loss: 0.3520\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3490 - val_loss: 0.4205\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3479 - val_loss: 0.3579\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3478 - val_loss: 0.3904\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3488 - val_loss: 0.3621\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 54us/sample - loss: 0.3438 - val_loss: 0.3914\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 53us/sample - loss: 0.3462 - val_loss: 0.3555\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3399 - val_loss: 0.3616\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3396 - val_loss: 0.3996\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3372 - val_loss: 0.3572\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3421 - val_loss: 0.4385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ApfLufFWAXV",
        "colab_type": "text"
      },
      "source": [
        "Plot Learning curves of each model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KG7Kg1Aib22",
        "colab_type": "code",
        "outputId": "3db01a0f-0f8e-4b60-a795-5bbf4cf28fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for learning_rate, history in zip(learning_rates, histories):\n",
        "    print(\"Learning rate:\", learning_rate)\n",
        "    plot_learning_curves(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.0001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwBTGIiIiAfv/ub7JNCYOqXwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhM0kGAshhBA2k2AshBBC2Oz/A7zf4fcREV1iAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYxEREY8pjEVERDz2/wFlC3OvfpwyBgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNEy0ayYWn8D",
        "colab_type": "text"
      },
      "source": [
        "Slightly elegant way. Define function which will take set of hyperparameters, return compiled model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3SY_vJ-imKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3):\n",
        "\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Dense(n_neurons, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    ])\n",
        "\n",
        "    for i in range(n_hidden):\n",
        "      model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    \n",
        "    # Last layer\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    optimizer = keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YsDAvLRXIZG",
        "colab_type": "text"
      },
      "source": [
        "Use sklearn wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--lU-FV8kXmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdySDR1QXvJX",
        "colab_type": "text"
      },
      "source": [
        "Fit model with sklearns wrapper. Set callbacks and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpweZb-kkHU",
        "colab_type": "code",
        "outputId": "1c0e67e1-fc59-4aa3-bb2a-d45626343e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "keras_reg.fit(X_train_scaled, y_train, epochs=100,\n",
        "              validation_data=(X_val_scaled, y_val),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 1s 90us/sample - loss: 1.2426 - val_loss: 0.8138\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.6043 - val_loss: 0.5867\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.5131 - val_loss: 0.4790\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.4629 - val_loss: 0.4464\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.4364 - val_loss: 0.4215\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.4156 - val_loss: 0.5186\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.4045 - val_loss: 0.4150\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3936 - val_loss: 0.3950\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3843 - val_loss: 0.4044\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3781 - val_loss: 0.4172\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3735 - val_loss: 0.3945\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3684 - val_loss: 0.3707\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 55us/sample - loss: 0.3636 - val_loss: 0.3711\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.3603 - val_loss: 0.3959\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3576 - val_loss: 0.3609\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3542 - val_loss: 0.3986\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3525 - val_loss: 0.3549\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3504 - val_loss: 0.3523\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3481 - val_loss: 0.3525\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3462 - val_loss: 0.3633\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3448 - val_loss: 0.3482\n",
            "Epoch 22/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3429 - val_loss: 0.3542\n",
            "Epoch 23/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3422 - val_loss: 0.3507\n",
            "Epoch 24/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3405 - val_loss: 0.3485\n",
            "Epoch 25/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3394 - val_loss: 0.3475\n",
            "Epoch 26/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3380 - val_loss: 0.3465\n",
            "Epoch 27/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3376 - val_loss: 0.3430\n",
            "Epoch 28/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3365 - val_loss: 0.3454\n",
            "Epoch 29/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3353 - val_loss: 0.3424\n",
            "Epoch 30/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3346 - val_loss: 0.3396\n",
            "Epoch 31/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3329 - val_loss: 0.3495\n",
            "Epoch 32/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3325 - val_loss: 0.3671\n",
            "Epoch 33/100\n",
            "9907/9907 [==============================] - 1s 69us/sample - loss: 0.3314 - val_loss: 0.3601\n",
            "Epoch 34/100\n",
            "9907/9907 [==============================] - 1s 64us/sample - loss: 0.3309 - val_loss: 0.3509\n",
            "Epoch 35/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3291 - val_loss: 0.3367\n",
            "Epoch 36/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3283 - val_loss: 0.3353\n",
            "Epoch 37/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3274 - val_loss: 0.3453\n",
            "Epoch 38/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3262 - val_loss: 0.3339\n",
            "Epoch 39/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3255 - val_loss: 0.3336\n",
            "Epoch 40/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3247 - val_loss: 0.3359\n",
            "Epoch 41/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3237 - val_loss: 0.3322\n",
            "Epoch 42/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3229 - val_loss: 0.3368\n",
            "Epoch 43/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3226 - val_loss: 0.3300\n",
            "Epoch 44/100\n",
            "9907/9907 [==============================] - 1s 65us/sample - loss: 0.3214 - val_loss: 0.3314\n",
            "Epoch 45/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3198 - val_loss: 0.3296\n",
            "Epoch 46/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3195 - val_loss: 0.3304\n",
            "Epoch 47/100\n",
            "9907/9907 [==============================] - 1s 64us/sample - loss: 0.3189 - val_loss: 0.3327\n",
            "Epoch 48/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3177 - val_loss: 0.3535\n",
            "Epoch 49/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3170 - val_loss: 0.3336\n",
            "Epoch 50/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3165 - val_loss: 0.3427\n",
            "Epoch 51/100\n",
            "9907/9907 [==============================] - 1s 64us/sample - loss: 0.3159 - val_loss: 0.3309\n",
            "Epoch 52/100\n",
            "9907/9907 [==============================] - 1s 67us/sample - loss: 0.3150 - val_loss: 0.3261\n",
            "Epoch 53/100\n",
            "9907/9907 [==============================] - 1s 65us/sample - loss: 0.3144 - val_loss: 0.3244\n",
            "Epoch 54/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3134 - val_loss: 0.3266\n",
            "Epoch 55/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3127 - val_loss: 0.3287\n",
            "Epoch 56/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3125 - val_loss: 0.3305\n",
            "Epoch 57/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3121 - val_loss: 0.3360\n",
            "Epoch 58/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3111 - val_loss: 0.3246\n",
            "Epoch 59/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3101 - val_loss: 0.3270\n",
            "Epoch 60/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3106 - val_loss: 0.3523\n",
            "Epoch 61/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.3092 - val_loss: 0.3253\n",
            "Epoch 62/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3083 - val_loss: 0.3224\n",
            "Epoch 63/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3090 - val_loss: 0.3586\n",
            "Epoch 64/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3081 - val_loss: 0.3263\n",
            "Epoch 65/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3076 - val_loss: 0.3429\n",
            "Epoch 66/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3064 - val_loss: 0.3223\n",
            "Epoch 67/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3070 - val_loss: 0.3337\n",
            "Epoch 68/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3059 - val_loss: 0.3321\n",
            "Epoch 69/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3055 - val_loss: 0.3236\n",
            "Epoch 70/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.3044 - val_loss: 0.3546\n",
            "Epoch 71/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3049 - val_loss: 0.3602\n",
            "Epoch 72/100\n",
            "9907/9907 [==============================] - 1s 70us/sample - loss: 0.3044 - val_loss: 0.3183\n",
            "Epoch 73/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.3035 - val_loss: 0.3264\n",
            "Epoch 74/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3034 - val_loss: 0.3516\n",
            "Epoch 75/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3028 - val_loss: 0.3180\n",
            "Epoch 76/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3018 - val_loss: 0.3179\n",
            "Epoch 77/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.3016 - val_loss: 0.3202\n",
            "Epoch 78/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.3010 - val_loss: 0.3485\n",
            "Epoch 79/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3008 - val_loss: 0.3607\n",
            "Epoch 80/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.3005 - val_loss: 0.3234\n",
            "Epoch 81/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.3004 - val_loss: 0.3319\n",
            "Epoch 82/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.2993 - val_loss: 0.3134\n",
            "Epoch 83/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.2990 - val_loss: 0.3183\n",
            "Epoch 84/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.2989 - val_loss: 0.3230\n",
            "Epoch 85/100\n",
            "9907/9907 [==============================] - 1s 67us/sample - loss: 0.2977 - val_loss: 0.3347\n",
            "Epoch 86/100\n",
            "9907/9907 [==============================] - 1s 56us/sample - loss: 0.2978 - val_loss: 0.3115\n",
            "Epoch 87/100\n",
            "9907/9907 [==============================] - 1s 60us/sample - loss: 0.2971 - val_loss: 0.3270\n",
            "Epoch 88/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.2971 - val_loss: 0.3123\n",
            "Epoch 89/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.2966 - val_loss: 0.3183\n",
            "Epoch 90/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.2963 - val_loss: 0.3221\n",
            "Epoch 91/100\n",
            "9907/9907 [==============================] - 1s 61us/sample - loss: 0.2954 - val_loss: 0.3146\n",
            "Epoch 92/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.2945 - val_loss: 0.3298\n",
            "Epoch 93/100\n",
            "9907/9907 [==============================] - 1s 58us/sample - loss: 0.2945 - val_loss: 0.3169\n",
            "Epoch 94/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.2938 - val_loss: 0.3231\n",
            "Epoch 95/100\n",
            "9907/9907 [==============================] - 1s 57us/sample - loss: 0.2935 - val_loss: 0.3105\n",
            "Epoch 96/100\n",
            "9907/9907 [==============================] - 1s 59us/sample - loss: 0.2933 - val_loss: 0.3240\n",
            "Epoch 97/100\n",
            "9907/9907 [==============================] - 1s 64us/sample - loss: 0.2926 - val_loss: 0.3096\n",
            "Epoch 98/100\n",
            "9907/9907 [==============================] - 1s 62us/sample - loss: 0.2925 - val_loss: 0.3147\n",
            "Epoch 99/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.2924 - val_loss: 0.3086\n",
            "Epoch 100/100\n",
            "9907/9907 [==============================] - 1s 63us/sample - loss: 0.2920 - val_loss: 0.3083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f26c7af3668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYTQVIAIlf9P",
        "colab_type": "code",
        "outputId": "1dc7002d-30ba-443e-f82d-b163e60d83d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "keras_reg.predict(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.6166496 , 3.1056743 , 0.89100957, ..., 3.5470285 , 1.2265637 ,\n",
              "       1.484509  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QG0CqAIYEm7",
        "colab_type": "text"
      },
      "source": [
        "Define set of hyperparameters to search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FheHmjq3ltVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(10, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiCfiVByYOIn",
        "colab_type": "text"
      },
      "source": [
        "Use Skearns RandomizedsearchCV for hyperparameter tuning with 3 fold CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiWgKMPAl6BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH_nMFmPYvnK",
        "colab_type": "text"
      },
      "source": [
        "Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj0OyMQLmMYP",
        "colab_type": "code",
        "outputId": "81d8dc4d-da64-43aa-974a-c50ec8be24d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rnd_search_cv.fit(X_train_scaled, y_train, epochs=100,\n",
        "                  validation_data=(X_val_scaled, y_val),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] learning_rate=0.0024851852868302353, n_hidden=1, n_neurons=51 ...\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6604/6604 [==============================] - 1s 126us/sample - loss: 1.1982 - val_loss: 0.7157\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.6605 - val_loss: 0.6204\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.5815 - val_loss: 0.5654\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.5253 - val_loss: 0.5139\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.4844 - val_loss: 0.4858\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.4547 - val_loss: 0.4898\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.4342 - val_loss: 0.4913\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.4195 - val_loss: 0.4788\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.4077 - val_loss: 0.4755\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3991 - val_loss: 0.4762\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3913 - val_loss: 0.4690\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3862 - val_loss: 0.4716\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3817 - val_loss: 0.4832\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3773 - val_loss: 0.4832\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 0s 66us/sample - loss: 0.3735 - val_loss: 0.4393\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3702 - val_loss: 0.4683\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3672 - val_loss: 0.4341\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3648 - val_loss: 0.4135\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 0s 66us/sample - loss: 0.3627 - val_loss: 0.4392\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 0s 66us/sample - loss: 0.3608 - val_loss: 0.4409\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3586 - val_loss: 0.4425\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3565 - val_loss: 0.4473\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3553 - val_loss: 0.4839\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3532 - val_loss: 0.3913\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3521 - val_loss: 0.4411\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3506 - val_loss: 0.4186\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 0s 66us/sample - loss: 0.3494 - val_loss: 0.4194\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3479 - val_loss: 0.4379\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 0s 65us/sample - loss: 0.3467 - val_loss: 0.3829\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 0s 65us/sample - loss: 0.3458 - val_loss: 0.4195\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3448 - val_loss: 0.4226\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 0s 65us/sample - loss: 0.3429 - val_loss: 0.4027\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3423 - val_loss: 0.3845\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3419 - val_loss: 0.4099\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3406 - val_loss: 0.4066\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3398 - val_loss: 0.3936\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3388 - val_loss: 0.3900\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3382 - val_loss: 0.3867\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3372 - val_loss: 0.3888\n",
            "3303/3303 [==============================] - 0s 25us/sample - loss: 0.3287\n",
            "[CV]  learning_rate=0.0024851852868302353, n_hidden=1, n_neurons=51, total=  18.5s\n",
            "[CV] learning_rate=0.0024851852868302353, n_hidden=1, n_neurons=51 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6605/6605 [==============================] - 1s 138us/sample - loss: 1.9335 - val_loss: 2.0503\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.6779 - val_loss: 0.6271\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.5634 - val_loss: 0.5264\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.5032 - val_loss: 0.4881\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.4653 - val_loss: 0.4397\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.4400 - val_loss: 0.4208\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 0s 66us/sample - loss: 0.4221 - val_loss: 0.4187\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.4084 - val_loss: 0.4221\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3978 - val_loss: 0.4399\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.3893 - val_loss: 0.4540\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3826 - val_loss: 0.4625\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3767 - val_loss: 0.4915\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3722 - val_loss: 0.4796\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3671 - val_loss: 0.4898\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3644 - val_loss: 0.5069\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3612 - val_loss: 0.4830\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3586 - val_loss: 0.4700\n",
            "3302/3302 [==============================] - 0s 28us/sample - loss: 0.3578\n",
            "[CV]  learning_rate=0.0024851852868302353, n_hidden=1, n_neurons=51, total=   8.6s\n",
            "[CV] learning_rate=0.0024851852868302353, n_hidden=1, n_neurons=51 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 134us/sample - loss: 1.2212 - val_loss: 0.7656\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.6222 - val_loss: 0.5782\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.5349 - val_loss: 0.5418\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.4767 - val_loss: 0.4652\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.4389 - val_loss: 0.4735\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.4157 - val_loss: 0.4453\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3985 - val_loss: 0.4550\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3885 - val_loss: 0.4001\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.3783 - val_loss: 0.4347\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3737 - val_loss: 0.3857\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3677 - val_loss: 0.3808\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3614 - val_loss: 0.4142\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3596 - val_loss: 0.3842\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3546 - val_loss: 0.4531\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3532 - val_loss: 0.3704\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3495 - val_loss: 0.3626\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3460 - val_loss: 0.3867\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3444 - val_loss: 0.3678\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3415 - val_loss: 0.3657\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3398 - val_loss: 0.3807\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3381 - val_loss: 0.3731\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3363 - val_loss: 0.3767\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3347 - val_loss: 0.3638\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3333 - val_loss: 0.3534\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3312 - val_loss: 0.3564\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3299 - val_loss: 0.3484\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3287 - val_loss: 0.3951\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3275 - val_loss: 0.3501\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3262 - val_loss: 0.3461\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3250 - val_loss: 0.3451\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.3234 - val_loss: 0.3526\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3228 - val_loss: 0.3577\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3215 - val_loss: 0.3574\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3203 - val_loss: 0.3435\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.3196 - val_loss: 0.3578\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3183 - val_loss: 0.3374\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3172 - val_loss: 0.3410\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3165 - val_loss: 0.3401\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3156 - val_loss: 0.3360\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3143 - val_loss: 0.3428\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3130 - val_loss: 0.3368\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3124 - val_loss: 0.3505\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3117 - val_loss: 0.3421\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3105 - val_loss: 0.3346\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3100 - val_loss: 0.3465\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3084 - val_loss: 0.3391\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3078 - val_loss: 0.3421\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3076 - val_loss: 0.3374\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3071 - val_loss: 0.3301\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3058 - val_loss: 0.3574\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3050 - val_loss: 0.4335\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3050 - val_loss: 0.3330\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.3041 - val_loss: 0.3525\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3035 - val_loss: 0.3425\n",
            "Epoch 55/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3028 - val_loss: 0.3585\n",
            "Epoch 56/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.3021 - val_loss: 0.3823\n",
            "Epoch 57/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3015 - val_loss: 0.3320\n",
            "Epoch 58/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.3001 - val_loss: 0.3271\n",
            "Epoch 59/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3001 - val_loss: 0.3901\n",
            "Epoch 60/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2991 - val_loss: 0.3396\n",
            "Epoch 61/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2988 - val_loss: 0.3235\n",
            "Epoch 62/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.2981 - val_loss: 0.3532\n",
            "Epoch 63/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2980 - val_loss: 0.3260\n",
            "Epoch 64/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2973 - val_loss: 0.3477\n",
            "Epoch 65/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2967 - val_loss: 0.3241\n",
            "Epoch 66/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2956 - val_loss: 0.3210\n",
            "Epoch 67/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2954 - val_loss: 0.3375\n",
            "Epoch 68/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.2955 - val_loss: 0.3253\n",
            "Epoch 69/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.2943 - val_loss: 0.3916\n",
            "Epoch 70/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2941 - val_loss: 0.3206\n",
            "Epoch 71/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.2931 - val_loss: 0.3235\n",
            "Epoch 72/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.2929 - val_loss: 0.3252\n",
            "Epoch 73/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2922 - val_loss: 0.4166\n",
            "Epoch 74/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.2921 - val_loss: 0.3252\n",
            "Epoch 75/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2917 - val_loss: 0.3195\n",
            "Epoch 76/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.2910 - val_loss: 0.3203\n",
            "Epoch 77/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2901 - val_loss: 0.3215\n",
            "Epoch 78/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.2908 - val_loss: 0.3732\n",
            "Epoch 79/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2895 - val_loss: 0.3267\n",
            "Epoch 80/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.2891 - val_loss: 0.3183\n",
            "Epoch 81/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2884 - val_loss: 0.3580\n",
            "Epoch 82/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2882 - val_loss: 0.3203\n",
            "Epoch 83/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2880 - val_loss: 0.3367\n",
            "Epoch 84/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.2867 - val_loss: 0.3164\n",
            "Epoch 85/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2858 - val_loss: 0.3225\n",
            "Epoch 86/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2867 - val_loss: 0.3272\n",
            "Epoch 87/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2866 - val_loss: 0.3718\n",
            "Epoch 88/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2856 - val_loss: 0.3400\n",
            "Epoch 89/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.2852 - val_loss: 0.3237\n",
            "Epoch 90/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.2842 - val_loss: 0.3689\n",
            "Epoch 91/100\n",
            "6605/6605 [==============================] - 0s 67us/sample - loss: 0.2845 - val_loss: 0.3247\n",
            "Epoch 92/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.2839 - val_loss: 0.3164\n",
            "Epoch 93/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2833 - val_loss: 0.3155\n",
            "Epoch 94/100\n",
            "6605/6605 [==============================] - 0s 66us/sample - loss: 0.2830 - val_loss: 0.3665\n",
            "Epoch 95/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.2823 - val_loss: 0.3175\n",
            "Epoch 96/100\n",
            "6605/6605 [==============================] - 0s 66us/sample - loss: 0.2819 - val_loss: 0.3257\n",
            "Epoch 97/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2815 - val_loss: 0.3264\n",
            "Epoch 98/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2817 - val_loss: 0.3353\n",
            "Epoch 99/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.2808 - val_loss: 0.3113\n",
            "Epoch 100/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2802 - val_loss: 0.3132\n",
            "3302/3302 [==============================] - 0s 25us/sample - loss: 0.3284\n",
            "[CV]  learning_rate=0.0024851852868302353, n_hidden=1, n_neurons=51, total=  47.1s\n",
            "[CV] learning_rate=0.00980577510814503, n_hidden=2, n_neurons=39 .....\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 1s 127us/sample - loss: 0.7073 - val_loss: 1.6791\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.4486 - val_loss: 2.6033\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.4274 - val_loss: 4.6479\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.4107 - val_loss: 0.5476\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3675 - val_loss: 0.3526\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3611 - val_loss: 0.3641\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.3510 - val_loss: 0.3718\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3468 - val_loss: 0.3582\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3425 - val_loss: 0.3406\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3371 - val_loss: 0.3411\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3351 - val_loss: 0.3404\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3325 - val_loss: 0.3708\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3327 - val_loss: 0.3318\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3288 - val_loss: 0.3370\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3266 - val_loss: 0.3333\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3228 - val_loss: 0.3609\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3227 - val_loss: 0.3804\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3187 - val_loss: 0.3346\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3174 - val_loss: 0.3316\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3153 - val_loss: 0.3258\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3109 - val_loss: 0.3489\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3118 - val_loss: 0.3559\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3106 - val_loss: 0.3221\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3079 - val_loss: 0.3425\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3070 - val_loss: 0.3163\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3054 - val_loss: 0.3441\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3022 - val_loss: 0.3212\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.2998 - val_loss: 0.3646\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.2988 - val_loss: 0.3283\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.2976 - val_loss: 0.3318\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.2982 - val_loss: 0.3124\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.2944 - val_loss: 0.3225\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.2951 - val_loss: 0.4183\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.2924 - val_loss: 0.3113\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.2897 - val_loss: 0.3151\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.2862 - val_loss: 0.3522\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.2887 - val_loss: 0.3485\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.2877 - val_loss: 0.3431\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.2914 - val_loss: 0.3208\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.2857 - val_loss: 0.3274\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.2841 - val_loss: 0.3705\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.2851 - val_loss: 0.3991\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 0s 67us/sample - loss: 0.2799 - val_loss: 0.3066\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.2797 - val_loss: 0.3414\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.2765 - val_loss: 0.3233\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.2771 - val_loss: 0.4031\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.2766 - val_loss: 0.3162\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.2761 - val_loss: 0.3238\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.2739 - val_loss: 0.3257\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.2723 - val_loss: 0.3338\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.2726 - val_loss: 0.3460\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.2713 - val_loss: 0.3139\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.2679 - val_loss: 0.3172\n",
            "3303/3303 [==============================] - 0s 26us/sample - loss: 0.2965\n",
            "[CV]  learning_rate=0.00980577510814503, n_hidden=2, n_neurons=39, total=  25.8s\n",
            "[CV] learning_rate=0.00980577510814503, n_hidden=2, n_neurons=39 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 166us/sample - loss: 0.8796 - val_loss: 0.4888\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.4284 - val_loss: 0.4472\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3857 - val_loss: 0.5407\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3671 - val_loss: 0.6941\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3575 - val_loss: 0.7939\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3515 - val_loss: 0.6090\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3431 - val_loss: 0.6594\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3376 - val_loss: 0.6261\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3353 - val_loss: 0.5433\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3307 - val_loss: 0.6052\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3276 - val_loss: 0.5097\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3254 - val_loss: 0.5786\n",
            "3302/3302 [==============================] - 0s 26us/sample - loss: 0.3383\n",
            "[CV]  learning_rate=0.00980577510814503, n_hidden=2, n_neurons=39, total=   6.7s\n",
            "[CV] learning_rate=0.00980577510814503, n_hidden=2, n_neurons=39 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 160us/sample - loss: 0.7665 - val_loss: 0.5051\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.4327 - val_loss: 0.4568\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3822 - val_loss: 0.4406\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3598 - val_loss: 0.4213\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3463 - val_loss: 0.4901\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3407 - val_loss: 0.3620\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3360 - val_loss: 0.3823\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3299 - val_loss: 0.3815\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3252 - val_loss: 0.3498\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3229 - val_loss: 0.3627\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3194 - val_loss: 0.3530\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3164 - val_loss: 0.3437\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3150 - val_loss: 0.3869\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3125 - val_loss: 0.3386\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3094 - val_loss: 0.3414\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3056 - val_loss: 0.3996\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3059 - val_loss: 0.3478\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3016 - val_loss: 0.3346\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.2999 - val_loss: 0.3408\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.2990 - val_loss: 0.3464\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2960 - val_loss: 0.3250\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 0s 76us/sample - loss: 0.2947 - val_loss: 0.3804\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2907 - val_loss: 0.3658\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.2906 - val_loss: 0.3434\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.2897 - val_loss: 0.3258\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.2873 - val_loss: 0.3296\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.2836 - val_loss: 0.3263\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.2822 - val_loss: 0.3256\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.2831 - val_loss: 0.3386\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.2804 - val_loss: 0.3254\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2775 - val_loss: 0.3475\n",
            "3302/3302 [==============================] - 0s 26us/sample - loss: 0.3274\n",
            "[CV]  learning_rate=0.00980577510814503, n_hidden=2, n_neurons=39, total=  16.4s\n",
            "[CV] learning_rate=0.001857140245271595, n_hidden=1, n_neurons=46 ....\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 1s 143us/sample - loss: 1.7077 - val_loss: 0.7987\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.6855 - val_loss: 0.6286\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.5922 - val_loss: 0.5542\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.5415 - val_loss: 0.5118\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.5054 - val_loss: 0.4861\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.4786 - val_loss: 0.4811\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.4585 - val_loss: 0.4929\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.4434 - val_loss: 0.4448\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.4300 - val_loss: 0.4418\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.4204 - val_loss: 0.4261\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.4122 - val_loss: 0.4171\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.4055 - val_loss: 0.4143\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3998 - val_loss: 0.4121\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3945 - val_loss: 0.4114\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3908 - val_loss: 0.4076\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3866 - val_loss: 0.3919\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3839 - val_loss: 0.3973\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3811 - val_loss: 0.3864\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3782 - val_loss: 0.3916\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3754 - val_loss: 0.3776\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3734 - val_loss: 0.3789\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3715 - val_loss: 0.3718\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3689 - val_loss: 0.3811\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3676 - val_loss: 0.3718\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3660 - val_loss: 0.3672\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3646 - val_loss: 0.3662\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3631 - val_loss: 0.3639\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3615 - val_loss: 0.3720\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3604 - val_loss: 0.3746\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3592 - val_loss: 0.3732\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3581 - val_loss: 0.3651\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3564 - val_loss: 0.3654\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3554 - val_loss: 0.3604\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3546 - val_loss: 0.3628\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3537 - val_loss: 0.3705\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3529 - val_loss: 0.3767\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3519 - val_loss: 0.3587\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3504 - val_loss: 0.3534\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3498 - val_loss: 0.3519\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3489 - val_loss: 0.3616\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3483 - val_loss: 0.3547\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3469 - val_loss: 0.3529\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3466 - val_loss: 0.3559\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3455 - val_loss: 0.3547\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3447 - val_loss: 0.3536\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3437 - val_loss: 0.3518\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3433 - val_loss: 0.3537\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3424 - val_loss: 0.3607\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3417 - val_loss: 0.3471\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3408 - val_loss: 0.3532\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3402 - val_loss: 0.3459\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3397 - val_loss: 0.3495\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3393 - val_loss: 0.3446\n",
            "Epoch 54/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3386 - val_loss: 0.3499\n",
            "Epoch 55/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3376 - val_loss: 0.3481\n",
            "Epoch 56/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3371 - val_loss: 0.3701\n",
            "Epoch 57/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3365 - val_loss: 0.3473\n",
            "Epoch 58/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3361 - val_loss: 0.3548\n",
            "Epoch 59/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3355 - val_loss: 0.3462\n",
            "Epoch 60/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3347 - val_loss: 0.3538\n",
            "Epoch 61/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3344 - val_loss: 0.3480\n",
            "Epoch 62/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3336 - val_loss: 0.3427\n",
            "Epoch 63/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3333 - val_loss: 0.3480\n",
            "Epoch 64/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3327 - val_loss: 0.3419\n",
            "Epoch 65/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3322 - val_loss: 0.3519\n",
            "Epoch 66/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3319 - val_loss: 0.3455\n",
            "Epoch 67/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3312 - val_loss: 0.3417\n",
            "Epoch 68/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3306 - val_loss: 0.3441\n",
            "Epoch 69/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3303 - val_loss: 0.3427\n",
            "Epoch 70/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3300 - val_loss: 0.3398\n",
            "Epoch 71/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3291 - val_loss: 0.3455\n",
            "Epoch 72/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3290 - val_loss: 0.3479\n",
            "Epoch 73/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3284 - val_loss: 0.3419\n",
            "Epoch 74/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3281 - val_loss: 0.3422\n",
            "Epoch 75/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3275 - val_loss: 0.3407\n",
            "Epoch 76/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3271 - val_loss: 0.3423\n",
            "Epoch 77/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3262 - val_loss: 0.3423\n",
            "Epoch 78/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3258 - val_loss: 0.3373\n",
            "Epoch 79/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3256 - val_loss: 0.3492\n",
            "Epoch 80/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3251 - val_loss: 0.3481\n",
            "Epoch 81/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3247 - val_loss: 0.3549\n",
            "Epoch 82/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3245 - val_loss: 0.3400\n",
            "Epoch 83/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3241 - val_loss: 0.3370\n",
            "Epoch 84/100\n",
            "6604/6604 [==============================] - 0s 69us/sample - loss: 0.3238 - val_loss: 0.3417\n",
            "Epoch 85/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3235 - val_loss: 0.3374\n",
            "Epoch 86/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3228 - val_loss: 0.3342\n",
            "Epoch 87/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3223 - val_loss: 0.3387\n",
            "Epoch 88/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3222 - val_loss: 0.3445\n",
            "Epoch 89/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3217 - val_loss: 0.3381\n",
            "Epoch 90/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3212 - val_loss: 0.3432\n",
            "Epoch 91/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3214 - val_loss: 0.3487\n",
            "Epoch 92/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3205 - val_loss: 0.3500\n",
            "Epoch 93/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3205 - val_loss: 0.3338\n",
            "Epoch 94/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3198 - val_loss: 0.3371\n",
            "Epoch 95/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3197 - val_loss: 0.3453\n",
            "Epoch 96/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3196 - val_loss: 0.3424\n",
            "Epoch 97/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3188 - val_loss: 0.3450\n",
            "Epoch 98/100\n",
            "6604/6604 [==============================] - 0s 68us/sample - loss: 0.3189 - val_loss: 0.3446\n",
            "Epoch 99/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3180 - val_loss: 0.3628\n",
            "Epoch 100/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3182 - val_loss: 0.3387\n",
            "3303/3303 [==============================] - 0s 25us/sample - loss: 0.3150\n",
            "[CV]  learning_rate=0.001857140245271595, n_hidden=1, n_neurons=46, total=  48.4s\n",
            "[CV] learning_rate=0.001857140245271595, n_hidden=1, n_neurons=46 ....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 167us/sample - loss: 1.7363 - val_loss: 3.7366\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.7681 - val_loss: 1.2398\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.6783 - val_loss: 0.8517\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.6220 - val_loss: 0.6890\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.5776 - val_loss: 0.6238\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.5422 - val_loss: 0.5724\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.5137 - val_loss: 0.5676\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.4906 - val_loss: 0.5665\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 0s 68us/sample - loss: 0.4711 - val_loss: 0.5623\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.4551 - val_loss: 0.5846\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.4417 - val_loss: 0.5986\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.4300 - val_loss: 0.6087\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.4203 - val_loss: 0.6238\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.4116 - val_loss: 0.6412\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.4042 - val_loss: 0.6258\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3973 - val_loss: 0.6454\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3920 - val_loss: 0.6540\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3864 - val_loss: 0.6583\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3822 - val_loss: 0.6555\n",
            "3302/3302 [==============================] - 0s 25us/sample - loss: 0.3802\n",
            "[CV]  learning_rate=0.001857140245271595, n_hidden=1, n_neurons=46, total=  10.0s\n",
            "[CV] learning_rate=0.001857140245271595, n_hidden=1, n_neurons=46 ....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 155us/sample - loss: 1.4340 - val_loss: 0.8419\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.7137 - val_loss: 0.6703\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.6221 - val_loss: 0.5884\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.5565 - val_loss: 0.5520\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.5059 - val_loss: 0.5046\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.4681 - val_loss: 0.4797\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.4394 - val_loss: 0.4635\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.4194 - val_loss: 0.4447\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.4040 - val_loss: 0.4200\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3931 - val_loss: 0.4266\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3846 - val_loss: 0.4119\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3782 - val_loss: 0.4011\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3726 - val_loss: 0.3986\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3674 - val_loss: 0.3870\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3645 - val_loss: 0.3849\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3615 - val_loss: 0.3767\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3583 - val_loss: 0.3783\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3563 - val_loss: 0.3731\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3539 - val_loss: 0.3749\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3525 - val_loss: 0.3711\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3505 - val_loss: 0.3684\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3489 - val_loss: 0.3653\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3476 - val_loss: 0.3704\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3454 - val_loss: 0.3699\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3443 - val_loss: 0.3626\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3436 - val_loss: 0.3646\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3416 - val_loss: 0.3610\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3412 - val_loss: 0.3695\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3398 - val_loss: 0.3674\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3386 - val_loss: 0.3635\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3374 - val_loss: 0.3619\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3369 - val_loss: 0.3651\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3359 - val_loss: 0.3648\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3348 - val_loss: 0.3869\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3337 - val_loss: 0.3552\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3333 - val_loss: 0.3646\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3325 - val_loss: 0.3722\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3316 - val_loss: 0.3756\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3301 - val_loss: 0.3747\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 0s 70us/sample - loss: 0.3302 - val_loss: 0.4060\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 0s 69us/sample - loss: 0.3289 - val_loss: 0.3915\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3284 - val_loss: 0.3851\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3269 - val_loss: 0.3826\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3268 - val_loss: 0.3894\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3259 - val_loss: 0.4030\n",
            "3302/3302 [==============================] - 0s 26us/sample - loss: 0.3677\n",
            "[CV]  learning_rate=0.001857140245271595, n_hidden=1, n_neurons=46, total=  22.6s\n",
            "[CV] learning_rate=0.0063571589882495845, n_hidden=3, n_neurons=16 ...\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 1s 163us/sample - loss: 1.0578 - val_loss: 0.6381\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.5393 - val_loss: 0.5132\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.4649 - val_loss: 0.4560\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.4317 - val_loss: 0.4255\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.4126 - val_loss: 0.4314\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3991 - val_loss: 0.4627\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3907 - val_loss: 0.4473\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3813 - val_loss: 0.4128\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3757 - val_loss: 0.4170\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3706 - val_loss: 0.3706\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3646 - val_loss: 0.3945\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3590 - val_loss: 0.3665\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3547 - val_loss: 0.3564\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3507 - val_loss: 0.3563\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3473 - val_loss: 0.3642\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3445 - val_loss: 0.3562\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 0s 76us/sample - loss: 0.3403 - val_loss: 0.3470\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3404 - val_loss: 0.3667\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3373 - val_loss: 0.3609\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3335 - val_loss: 0.3508\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3274 - val_loss: 0.3427\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3269 - val_loss: 0.3436\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3240 - val_loss: 0.3345\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3230 - val_loss: 0.3467\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3201 - val_loss: 0.3258\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3168 - val_loss: 0.3512\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 0s 70us/sample - loss: 0.3165 - val_loss: 0.4297\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 0s 71us/sample - loss: 0.3149 - val_loss: 0.3601\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3131 - val_loss: 0.3266\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3084 - val_loss: 0.3368\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 0s 72us/sample - loss: 0.3074 - val_loss: 0.3393\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.3050 - val_loss: 0.3406\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3050 - val_loss: 0.3289\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3034 - val_loss: 0.3421\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3013 - val_loss: 0.3317\n",
            "3303/3303 [==============================] - 0s 27us/sample - loss: 0.3223\n",
            "[CV]  learning_rate=0.0063571589882495845, n_hidden=3, n_neurons=16, total=  18.2s\n",
            "[CV] learning_rate=0.0063571589882495845, n_hidden=3, n_neurons=16 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 197us/sample - loss: 1.1459 - val_loss: 0.6335\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.5209 - val_loss: 0.4624\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.4362 - val_loss: 0.5104\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.4008 - val_loss: 0.6319\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3874 - val_loss: 0.6247\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3800 - val_loss: 0.6434\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3717 - val_loss: 0.5873\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3671 - val_loss: 0.5884\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3643 - val_loss: 0.5541\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3621 - val_loss: 0.5269\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3572 - val_loss: 0.4649\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3547 - val_loss: 0.4531\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3534 - val_loss: 0.4954\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3536 - val_loss: 0.4623\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3488 - val_loss: 0.4776\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3471 - val_loss: 0.4536\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3465 - val_loss: 0.4351\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3447 - val_loss: 0.4753\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3428 - val_loss: 0.3910\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3417 - val_loss: 0.3885\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3401 - val_loss: 0.4080\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3379 - val_loss: 0.4211\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3381 - val_loss: 0.3872\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3359 - val_loss: 0.3768\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3347 - val_loss: 0.3946\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3337 - val_loss: 0.3815\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3315 - val_loss: 0.3477\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3311 - val_loss: 0.3760\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3297 - val_loss: 0.3595\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3293 - val_loss: 0.3749\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3274 - val_loss: 0.3594\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3265 - val_loss: 0.3539\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3261 - val_loss: 0.3452\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3234 - val_loss: 0.3602\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3214 - val_loss: 0.3509\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3209 - val_loss: 0.3436\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3209 - val_loss: 0.3850\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3200 - val_loss: 0.3512\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3188 - val_loss: 0.3541\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3169 - val_loss: 0.3414\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 0s 71us/sample - loss: 0.3166 - val_loss: 0.3452\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 0s 76us/sample - loss: 0.3161 - val_loss: 0.3435\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3156 - val_loss: 0.3321\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3138 - val_loss: 0.3337\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3139 - val_loss: 0.3272\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3132 - val_loss: 0.3267\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3107 - val_loss: 0.3320\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3099 - val_loss: 0.3286\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3090 - val_loss: 0.3673\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3089 - val_loss: 0.3281\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3075 - val_loss: 0.3280\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3054 - val_loss: 0.3492\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3059 - val_loss: 0.3269\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3041 - val_loss: 0.3402\n",
            "Epoch 55/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3031 - val_loss: 0.3343\n",
            "Epoch 56/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3031 - val_loss: 0.3395\n",
            "3302/3302 [==============================] - 0s 28us/sample - loss: 0.3415\n",
            "[CV]  learning_rate=0.0063571589882495845, n_hidden=3, n_neurons=16, total=  28.9s\n",
            "[CV] learning_rate=0.0063571589882495845, n_hidden=3, n_neurons=16 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 172us/sample - loss: 1.1923 - val_loss: 0.7680\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.5224 - val_loss: 0.5514\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.4469 - val_loss: 0.4632\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.4124 - val_loss: 0.4179\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3908 - val_loss: 0.4076\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3778 - val_loss: 0.3966\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3707 - val_loss: 0.3944\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3643 - val_loss: 0.3837\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3595 - val_loss: 0.3777\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3550 - val_loss: 0.3961\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3544 - val_loss: 0.3699\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3485 - val_loss: 0.3674\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3472 - val_loss: 0.3651\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3440 - val_loss: 0.3707\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3432 - val_loss: 0.3615\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3398 - val_loss: 0.3703\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3386 - val_loss: 0.3616\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3364 - val_loss: 0.3659\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3361 - val_loss: 0.3646\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3327 - val_loss: 0.3563\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3321 - val_loss: 0.3587\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3305 - val_loss: 0.3639\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3316 - val_loss: 0.3531\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3306 - val_loss: 0.3527\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3281 - val_loss: 0.3588\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3259 - val_loss: 0.3543\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3253 - val_loss: 0.3558\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3232 - val_loss: 0.3489\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3230 - val_loss: 0.3485\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 0s 72us/sample - loss: 0.3211 - val_loss: 0.3614\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3210 - val_loss: 0.3582\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3197 - val_loss: 0.3547\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3192 - val_loss: 0.3491\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3174 - val_loss: 0.3425\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3151 - val_loss: 0.3436\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3160 - val_loss: 0.3377\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3156 - val_loss: 0.3384\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 0s 73us/sample - loss: 0.3125 - val_loss: 0.3420\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 0s 76us/sample - loss: 0.3121 - val_loss: 0.3410\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3126 - val_loss: 0.3348\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3104 - val_loss: 0.3356\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3100 - val_loss: 0.3407\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3099 - val_loss: 0.3382\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3075 - val_loss: 0.3393\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3069 - val_loss: 0.3403\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3055 - val_loss: 0.3330\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3070 - val_loss: 0.3461\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3040 - val_loss: 0.3544\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3041 - val_loss: 0.3317\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3027 - val_loss: 0.3369\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3011 - val_loss: 0.3370\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3001 - val_loss: 0.3458\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.2996 - val_loss: 0.3422\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2992 - val_loss: 0.3385\n",
            "Epoch 55/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.2981 - val_loss: 0.3283\n",
            "Epoch 56/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.2965 - val_loss: 0.3277\n",
            "Epoch 57/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2983 - val_loss: 0.3357\n",
            "Epoch 58/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2955 - val_loss: 0.3343\n",
            "Epoch 59/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2936 - val_loss: 0.3309\n",
            "Epoch 60/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.2932 - val_loss: 0.3319\n",
            "Epoch 61/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2933 - val_loss: 0.3758\n",
            "Epoch 62/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2929 - val_loss: 0.3280\n",
            "Epoch 63/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2911 - val_loss: 0.3196\n",
            "Epoch 64/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.2902 - val_loss: 0.3311\n",
            "Epoch 65/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.2903 - val_loss: 0.3218\n",
            "Epoch 66/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.2888 - val_loss: 0.3211\n",
            "Epoch 67/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2883 - val_loss: 0.3320\n",
            "Epoch 68/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2882 - val_loss: 0.3246\n",
            "Epoch 69/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.2858 - val_loss: 0.3398\n",
            "Epoch 70/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.2866 - val_loss: 0.3274\n",
            "Epoch 71/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.2860 - val_loss: 0.3348\n",
            "Epoch 72/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2857 - val_loss: 0.3358\n",
            "Epoch 73/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2856 - val_loss: 0.3212\n",
            "3302/3302 [==============================] - 0s 33us/sample - loss: 0.3346\n",
            "[CV]  learning_rate=0.0063571589882495845, n_hidden=3, n_neurons=16, total=  38.3s\n",
            "[CV] learning_rate=0.0015378578315127117, n_hidden=3, n_neurons=25 ...\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 1s 219us/sample - loss: 2.4786 - val_loss: 2.5359\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.9632 - val_loss: 0.8117\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.6891 - val_loss: 0.6421\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.5753 - val_loss: 0.5672\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.5173 - val_loss: 0.5406\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.4850 - val_loss: 0.4967\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.4653 - val_loss: 0.4802\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.4516 - val_loss: 0.4670\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.4415 - val_loss: 0.4538\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.4333 - val_loss: 0.4360\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.4262 - val_loss: 0.4453\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.4211 - val_loss: 0.4263\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.4144 - val_loss: 0.4152\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.4098 - val_loss: 0.4187\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.4063 - val_loss: 0.4218\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.4021 - val_loss: 0.4011\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3982 - val_loss: 0.4064\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3947 - val_loss: 0.4006\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3915 - val_loss: 0.3945\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3887 - val_loss: 0.3936\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3857 - val_loss: 0.3900\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3832 - val_loss: 0.3957\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3809 - val_loss: 0.3866\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3790 - val_loss: 0.3895\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3763 - val_loss: 0.3843\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3745 - val_loss: 0.3827\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3722 - val_loss: 0.3882\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3703 - val_loss: 0.3809\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3682 - val_loss: 0.3947\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3672 - val_loss: 0.3893\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3657 - val_loss: 0.3756\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3638 - val_loss: 0.3686\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3624 - val_loss: 0.3827\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3609 - val_loss: 0.3863\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3602 - val_loss: 0.3837\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3586 - val_loss: 0.3680\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3575 - val_loss: 0.3640\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3562 - val_loss: 0.3841\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3546 - val_loss: 0.3719\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3541 - val_loss: 0.3682\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3533 - val_loss: 0.3726\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3520 - val_loss: 0.3682\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3510 - val_loss: 0.3768\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3500 - val_loss: 0.3614\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3493 - val_loss: 0.3636\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3486 - val_loss: 0.3606\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3477 - val_loss: 0.3619\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3469 - val_loss: 0.3608\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3459 - val_loss: 0.3584\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3451 - val_loss: 0.3644\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3438 - val_loss: 0.3596\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3433 - val_loss: 0.3597\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3429 - val_loss: 0.3690\n",
            "Epoch 54/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3423 - val_loss: 0.3629\n",
            "Epoch 55/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3413 - val_loss: 0.3619\n",
            "Epoch 56/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3410 - val_loss: 0.3571\n",
            "Epoch 57/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3400 - val_loss: 0.3644\n",
            "Epoch 58/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3395 - val_loss: 0.3528\n",
            "Epoch 59/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3388 - val_loss: 0.3723\n",
            "Epoch 60/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3382 - val_loss: 0.3656\n",
            "Epoch 61/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3374 - val_loss: 0.3694\n",
            "Epoch 62/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3373 - val_loss: 0.3579\n",
            "Epoch 63/100\n",
            "6604/6604 [==============================] - 0s 76us/sample - loss: 0.3370 - val_loss: 0.3736\n",
            "Epoch 64/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3358 - val_loss: 0.3670\n",
            "Epoch 65/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3352 - val_loss: 0.3673\n",
            "Epoch 66/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3349 - val_loss: 0.3510\n",
            "Epoch 67/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3343 - val_loss: 0.3609\n",
            "Epoch 68/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3339 - val_loss: 0.3702\n",
            "Epoch 69/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3340 - val_loss: 0.3664\n",
            "Epoch 70/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3326 - val_loss: 0.3783\n",
            "Epoch 71/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3327 - val_loss: 0.3675\n",
            "Epoch 72/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3320 - val_loss: 0.3618\n",
            "Epoch 73/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3312 - val_loss: 0.3626\n",
            "Epoch 74/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3309 - val_loss: 0.3570\n",
            "Epoch 75/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3307 - val_loss: 0.3688\n",
            "Epoch 76/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3300 - val_loss: 0.3539\n",
            "3303/3303 [==============================] - 0s 29us/sample - loss: 0.3279\n",
            "[CV]  learning_rate=0.0015378578315127117, n_hidden=3, n_neurons=25, total=  40.8s\n",
            "[CV] learning_rate=0.0015378578315127117, n_hidden=3, n_neurons=25 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 188us/sample - loss: 1.7415 - val_loss: 4.9424\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.6757 - val_loss: 1.8936\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.5749 - val_loss: 1.2919\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.5280 - val_loss: 1.0615\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.4963 - val_loss: 0.9428\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.4719 - val_loss: 0.9273\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.4535 - val_loss: 0.9049\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.4381 - val_loss: 0.9126\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.4264 - val_loss: 0.8960\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.4155 - val_loss: 0.9200\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.4061 - val_loss: 0.9605\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3986 - val_loss: 0.9442\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3911 - val_loss: 1.0165\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3856 - val_loss: 0.9931\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3808 - val_loss: 1.0060\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3767 - val_loss: 0.9401\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3726 - val_loss: 0.9873\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3691 - val_loss: 0.9510\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3663 - val_loss: 0.9121\n",
            "3302/3302 [==============================] - 0s 27us/sample - loss: 0.3737\n",
            "[CV]  learning_rate=0.0015378578315127117, n_hidden=3, n_neurons=25, total=  11.0s\n",
            "[CV] learning_rate=0.0015378578315127117, n_hidden=3, n_neurons=25 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 1s 198us/sample - loss: 1.9689 - val_loss: 1.3625\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.7903 - val_loss: 0.6761\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.6012 - val_loss: 0.5944\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 0s 76us/sample - loss: 0.5396 - val_loss: 0.5606\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.5012 - val_loss: 0.5726\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.4757 - val_loss: 0.4904\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.4539 - val_loss: 0.4698\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.4366 - val_loss: 0.4415\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.4219 - val_loss: 0.4323\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.4108 - val_loss: 0.4223\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.4003 - val_loss: 0.4086\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3925 - val_loss: 0.4048\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 0s 74us/sample - loss: 0.3851 - val_loss: 0.4024\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3808 - val_loss: 0.3957\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3753 - val_loss: 0.3897\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3707 - val_loss: 0.3846\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3664 - val_loss: 0.3914\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3625 - val_loss: 0.3896\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3602 - val_loss: 0.3775\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3568 - val_loss: 0.3788\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3540 - val_loss: 0.3755\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3514 - val_loss: 0.3872\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3490 - val_loss: 0.3798\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3470 - val_loss: 0.3830\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3440 - val_loss: 0.3736\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3430 - val_loss: 0.3708\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3414 - val_loss: 0.3914\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3390 - val_loss: 0.3616\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3369 - val_loss: 0.3764\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3361 - val_loss: 0.3683\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3341 - val_loss: 0.3682\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3322 - val_loss: 0.3682\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3313 - val_loss: 0.3575\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3297 - val_loss: 0.3576\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3292 - val_loss: 0.3616\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3282 - val_loss: 0.3525\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3260 - val_loss: 0.3666\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3260 - val_loss: 0.3582\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3251 - val_loss: 0.3544\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3241 - val_loss: 0.3687\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3233 - val_loss: 0.3479\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3212 - val_loss: 0.3614\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3209 - val_loss: 0.3564\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3187 - val_loss: 0.3509\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3189 - val_loss: 0.3465\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3176 - val_loss: 0.3520\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3173 - val_loss: 0.3443\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3165 - val_loss: 0.3482\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3145 - val_loss: 0.3508\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3142 - val_loss: 0.3475\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3139 - val_loss: 0.3401\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3126 - val_loss: 0.3414\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3122 - val_loss: 0.3613\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3113 - val_loss: 0.3392\n",
            "Epoch 55/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3108 - val_loss: 0.3401\n",
            "Epoch 56/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3093 - val_loss: 0.3430\n",
            "Epoch 57/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3086 - val_loss: 0.3546\n",
            "Epoch 58/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3085 - val_loss: 0.3422\n",
            "Epoch 59/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3073 - val_loss: 0.3503\n",
            "Epoch 60/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3070 - val_loss: 0.3420\n",
            "Epoch 61/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3066 - val_loss: 0.3372\n",
            "Epoch 62/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3062 - val_loss: 0.3391\n",
            "Epoch 63/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3050 - val_loss: 0.3425\n",
            "Epoch 64/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3045 - val_loss: 0.3344\n",
            "Epoch 65/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3044 - val_loss: 0.3354\n",
            "Epoch 66/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3032 - val_loss: 0.3394\n",
            "Epoch 67/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3025 - val_loss: 0.3342\n",
            "Epoch 68/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3021 - val_loss: 0.3378\n",
            "Epoch 69/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3014 - val_loss: 0.3667\n",
            "Epoch 70/100\n",
            "6605/6605 [==============================] - 0s 75us/sample - loss: 0.3007 - val_loss: 0.3648\n",
            "Epoch 71/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3001 - val_loss: 0.3386\n",
            "Epoch 72/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3001 - val_loss: 0.3352\n",
            "Epoch 73/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2996 - val_loss: 0.3356\n",
            "Epoch 74/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2986 - val_loss: 0.3330\n",
            "Epoch 75/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2979 - val_loss: 0.3607\n",
            "Epoch 76/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2966 - val_loss: 0.3370\n",
            "Epoch 77/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.2969 - val_loss: 0.3532\n",
            "Epoch 78/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.2964 - val_loss: 0.3572\n",
            "Epoch 79/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.2956 - val_loss: 0.3822\n",
            "Epoch 80/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2955 - val_loss: 0.3408\n",
            "Epoch 81/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2950 - val_loss: 0.3351\n",
            "Epoch 82/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2941 - val_loss: 0.3333\n",
            "Epoch 83/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2939 - val_loss: 0.3317\n",
            "Epoch 84/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2932 - val_loss: 0.3418\n",
            "Epoch 85/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.2925 - val_loss: 0.3391\n",
            "Epoch 86/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2921 - val_loss: 0.3323\n",
            "Epoch 87/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2916 - val_loss: 0.3420\n",
            "Epoch 88/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2905 - val_loss: 0.3747\n",
            "Epoch 89/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.2906 - val_loss: 0.3263\n",
            "Epoch 90/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2894 - val_loss: 0.3368\n",
            "Epoch 91/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2898 - val_loss: 0.3201\n",
            "Epoch 92/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2889 - val_loss: 0.3450\n",
            "Epoch 93/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2883 - val_loss: 0.3265\n",
            "Epoch 94/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2876 - val_loss: 0.3265\n",
            "Epoch 95/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2869 - val_loss: 0.3620\n",
            "Epoch 96/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2867 - val_loss: 0.3461\n",
            "Epoch 97/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2864 - val_loss: 0.3251\n",
            "Epoch 98/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2854 - val_loss: 0.3484\n",
            "Epoch 99/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.2850 - val_loss: 0.3309\n",
            "Epoch 100/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.2845 - val_loss: 0.3278\n",
            "3302/3302 [==============================] - 0s 29us/sample - loss: 0.3238\n",
            "[CV]  learning_rate=0.0015378578315127117, n_hidden=3, n_neurons=25, total=  53.1s\n",
            "[CV] learning_rate=0.00807519916447461, n_hidden=3, n_neurons=13 .....\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 1s 205us/sample - loss: 1.1574 - val_loss: 1.4820\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.5852 - val_loss: 1.1698\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.5016 - val_loss: 0.5952\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.4496 - val_loss: 0.4410\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.4214 - val_loss: 0.4448\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 0s 73us/sample - loss: 0.4049 - val_loss: 0.4321\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3956 - val_loss: 0.3986\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3878 - val_loss: 0.3870\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3824 - val_loss: 0.3795\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3793 - val_loss: 0.3816\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3751 - val_loss: 0.3861\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3706 - val_loss: 0.3655\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3698 - val_loss: 0.3650\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3657 - val_loss: 0.3656\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3619 - val_loss: 0.3830\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3594 - val_loss: 0.3797\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3580 - val_loss: 0.3540\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3540 - val_loss: 0.3535\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3524 - val_loss: 0.3561\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3505 - val_loss: 0.3549\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3470 - val_loss: 0.3592\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3470 - val_loss: 0.3604\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3448 - val_loss: 0.3523\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3426 - val_loss: 0.3495\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3423 - val_loss: 0.3783\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3400 - val_loss: 0.3447\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3377 - val_loss: 0.3604\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3368 - val_loss: 0.3492\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 0s 76us/sample - loss: 0.3376 - val_loss: 0.3457\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3351 - val_loss: 0.3623\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3335 - val_loss: 0.3582\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3338 - val_loss: 0.3862\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3314 - val_loss: 0.3465\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3310 - val_loss: 0.3644\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3302 - val_loss: 0.3421\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3294 - val_loss: 0.3614\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3268 - val_loss: 0.3806\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3268 - val_loss: 0.3542\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3262 - val_loss: 0.3885\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3242 - val_loss: 0.3375\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3247 - val_loss: 0.3472\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3232 - val_loss: 0.3510\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3212 - val_loss: 0.3752\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3225 - val_loss: 0.3650\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3198 - val_loss: 0.3373\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3195 - val_loss: 0.3775\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3192 - val_loss: 0.3572\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3174 - val_loss: 0.3558\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3160 - val_loss: 0.3455\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3178 - val_loss: 0.3667\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3147 - val_loss: 0.3433\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3140 - val_loss: 0.3936\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3144 - val_loss: 0.3482\n",
            "Epoch 54/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3127 - val_loss: 0.3937\n",
            "Epoch 55/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3123 - val_loss: 0.3631\n",
            "3303/3303 [==============================] - 0s 27us/sample - loss: 0.3225\n",
            "[CV]  learning_rate=0.00807519916447461, n_hidden=3, n_neurons=13, total=  30.2s\n",
            "[CV] learning_rate=0.00807519916447461, n_hidden=3, n_neurons=13 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 254us/sample - loss: 1.0783 - val_loss: 0.6368\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.5315 - val_loss: 0.7284\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.4694 - val_loss: 0.4622\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.4318 - val_loss: 0.4134\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.4103 - val_loss: 0.3940\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3960 - val_loss: 0.3948\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3857 - val_loss: 0.4161\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3814 - val_loss: 0.4309\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3749 - val_loss: 0.4573\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3723 - val_loss: 0.4549\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3670 - val_loss: 0.4155\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3618 - val_loss: 0.5095\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3598 - val_loss: 0.4103\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3569 - val_loss: 0.4046\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3551 - val_loss: 0.4572\n",
            "3302/3302 [==============================] - 0s 27us/sample - loss: 0.3643\n",
            "[CV]  learning_rate=0.00807519916447461, n_hidden=3, n_neurons=13, total=   9.5s\n",
            "[CV] learning_rate=0.00807519916447461, n_hidden=3, n_neurons=13 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 247us/sample - loss: 0.9861 - val_loss: 0.5695\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.5011 - val_loss: 0.4860\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.4433 - val_loss: 0.4623\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.4183 - val_loss: 0.4349\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.4036 - val_loss: 0.4230\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3907 - val_loss: 0.4201\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3808 - val_loss: 0.3968\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3730 - val_loss: 0.3909\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3677 - val_loss: 0.3870\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3636 - val_loss: 0.3925\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3586 - val_loss: 0.3839\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3554 - val_loss: 0.3738\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3509 - val_loss: 0.3705\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3477 - val_loss: 0.3758\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3463 - val_loss: 0.3678\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3432 - val_loss: 0.3638\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3399 - val_loss: 0.3626\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3369 - val_loss: 0.3607\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3364 - val_loss: 0.3721\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3377 - val_loss: 0.3834\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3316 - val_loss: 0.3763\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3312 - val_loss: 0.3732\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3293 - val_loss: 0.3534\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3255 - val_loss: 0.3498\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3259 - val_loss: 0.3480\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3233 - val_loss: 0.3569\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3210 - val_loss: 0.3439\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3210 - val_loss: 0.3444\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3200 - val_loss: 0.3496\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3174 - val_loss: 0.3597\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3174 - val_loss: 0.3390\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3149 - val_loss: 0.3394\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3129 - val_loss: 0.3932\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3170 - val_loss: 0.3581\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3126 - val_loss: 0.3399\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3110 - val_loss: 0.3502\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3120 - val_loss: 0.3376\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3090 - val_loss: 0.3562\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3082 - val_loss: 0.3807\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3089 - val_loss: 0.3336\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3064 - val_loss: 0.3394\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3077 - val_loss: 0.4000\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3059 - val_loss: 0.3527\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3051 - val_loss: 0.3453\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3040 - val_loss: 0.3324\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3028 - val_loss: 0.3309\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3006 - val_loss: 0.3665\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3028 - val_loss: 0.3300\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2999 - val_loss: 0.3283\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.2982 - val_loss: 0.3575\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3009 - val_loss: 0.3285\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2993 - val_loss: 0.3579\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.2981 - val_loss: 0.3350\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2989 - val_loss: 0.3380\n",
            "Epoch 55/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.2949 - val_loss: 0.3332\n",
            "Epoch 56/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.2944 - val_loss: 0.3324\n",
            "Epoch 57/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2953 - val_loss: 0.3339\n",
            "Epoch 58/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2926 - val_loss: 0.3699\n",
            "Epoch 59/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2919 - val_loss: 0.3629\n",
            "3302/3302 [==============================] - 0s 29us/sample - loss: 0.3721\n",
            "[CV]  learning_rate=0.00807519916447461, n_hidden=3, n_neurons=13, total=  33.3s\n",
            "[CV] learning_rate=0.01543734951823562, n_hidden=2, n_neurons=89 .....\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 2s 281us/sample - loss: 0.7254 - val_loss: 0.4296\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.4016 - val_loss: 0.4203\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3689 - val_loss: 0.6068\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3786 - val_loss: 0.4839\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3464 - val_loss: 0.3696\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3407 - val_loss: 0.4692\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3377 - val_loss: 0.7887\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3377 - val_loss: 1.0451\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3381 - val_loss: 0.4881\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3239 - val_loss: 0.3822\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3199 - val_loss: 0.3371\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3132 - val_loss: 0.3358\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3111 - val_loss: 0.3261\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3079 - val_loss: 0.3409\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3064 - val_loss: 0.3133\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3018 - val_loss: 0.3209\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2988 - val_loss: 0.3311\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2939 - val_loss: 0.3286\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2933 - val_loss: 0.3770\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2880 - val_loss: 0.3105\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2885 - val_loss: 0.3128\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.2837 - val_loss: 0.3268\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.2821 - val_loss: 0.3083\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2783 - val_loss: 0.3192\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2774 - val_loss: 0.3436\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2743 - val_loss: 0.3339\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2722 - val_loss: 0.3154\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2730 - val_loss: 0.3342\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2737 - val_loss: 0.3172\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2686 - val_loss: 0.3907\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.2657 - val_loss: 0.3521\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2641 - val_loss: 0.3093\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.2642 - val_loss: 0.3210\n",
            "3303/3303 [==============================] - 0s 30us/sample - loss: 0.2854\n",
            "[CV]  learning_rate=0.01543734951823562, n_hidden=2, n_neurons=89, total=  20.2s\n",
            "[CV] learning_rate=0.01543734951823562, n_hidden=2, n_neurons=89 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 237us/sample - loss: 0.7069 - val_loss: 0.6833\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3955 - val_loss: 0.7181\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3611 - val_loss: 0.3946\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3454 - val_loss: 0.6102\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3365 - val_loss: 0.3613\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3271 - val_loss: 0.6095\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3251 - val_loss: 0.3903\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3182 - val_loss: 0.4533\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3134 - val_loss: 0.4851\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3072 - val_loss: 0.3403\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3061 - val_loss: 0.3986\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3019 - val_loss: 0.5071\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2993 - val_loss: 0.3635\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.2951 - val_loss: 0.3446\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 95us/sample - loss: 0.2907 - val_loss: 0.6889\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2913 - val_loss: 0.3970\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.2878 - val_loss: 0.4751\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.2875 - val_loss: 0.3265\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2829 - val_loss: 0.4097\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2801 - val_loss: 0.4041\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2786 - val_loss: 0.4560\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2771 - val_loss: 0.3508\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2749 - val_loss: 0.4195\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2737 - val_loss: 0.3313\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2694 - val_loss: 0.3326\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2702 - val_loss: 0.3677\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2682 - val_loss: 0.3244\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2668 - val_loss: 0.7581\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2669 - val_loss: 0.3141\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2644 - val_loss: 0.4324\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2643 - val_loss: 0.3063\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.2622 - val_loss: 0.3645\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.2601 - val_loss: 0.5921\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2604 - val_loss: 0.3181\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2542 - val_loss: 0.3024\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2546 - val_loss: 0.2929\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2544 - val_loss: 0.3198\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2542 - val_loss: 0.3135\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2529 - val_loss: 0.3353\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2536 - val_loss: 0.3671\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2491 - val_loss: 0.8203\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2504 - val_loss: 0.4298\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2492 - val_loss: 0.2917\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2482 - val_loss: 0.3670\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2464 - val_loss: 0.8105\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2438 - val_loss: 0.2961\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2433 - val_loss: 0.4419\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2426 - val_loss: 0.3060\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2417 - val_loss: 0.7625\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.2397 - val_loss: 0.3299\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2424 - val_loss: 0.2995\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2384 - val_loss: 0.3534\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2369 - val_loss: 0.5955\n",
            "3302/3302 [==============================] - 0s 31us/sample - loss: 0.2931\n",
            "[CV]  learning_rate=0.01543734951823562, n_hidden=2, n_neurons=89, total=  31.4s\n",
            "[CV] learning_rate=0.01543734951823562, n_hidden=2, n_neurons=89 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 255us/sample - loss: 0.6877 - val_loss: 1.2181\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3862 - val_loss: 0.5565\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3553 - val_loss: 0.4859\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3421 - val_loss: 0.4262\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3334 - val_loss: 0.3782\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3284 - val_loss: 0.3491\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3204 - val_loss: 0.3614\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3148 - val_loss: 0.4187\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3154 - val_loss: 0.5208\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3090 - val_loss: 0.4809\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3037 - val_loss: 0.3228\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.2977 - val_loss: 0.3246\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2980 - val_loss: 0.3655\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2937 - val_loss: 0.5449\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2924 - val_loss: 0.4444\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.2869 - val_loss: 0.3962\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2849 - val_loss: 0.3548\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2826 - val_loss: 0.3520\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2788 - val_loss: 0.3386\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.2786 - val_loss: 0.6535\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.2729 - val_loss: 0.3237\n",
            "3302/3302 [==============================] - 0s 30us/sample - loss: 0.3210\n",
            "[CV]  learning_rate=0.01543734951823562, n_hidden=2, n_neurons=89, total=  13.4s\n",
            "[CV] learning_rate=0.00397171378356561, n_hidden=1, n_neurons=26 .....\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 2s 257us/sample - loss: 1.3501 - val_loss: 0.7565\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.6253 - val_loss: 0.5664\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.5440 - val_loss: 0.4978\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.4920 - val_loss: 0.4563\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.4570 - val_loss: 0.4454\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.4345 - val_loss: 0.4263\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.4176 - val_loss: 0.4202\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.4070 - val_loss: 0.4046\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3979 - val_loss: 0.4112\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3926 - val_loss: 0.3908\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3859 - val_loss: 0.3832\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3803 - val_loss: 0.3923\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3777 - val_loss: 0.3881\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3749 - val_loss: 0.3954\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3696 - val_loss: 0.3827\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3671 - val_loss: 0.3712\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3635 - val_loss: 0.3914\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3623 - val_loss: 0.3653\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3596 - val_loss: 0.3627\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3571 - val_loss: 0.3657\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3556 - val_loss: 0.3603\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3543 - val_loss: 0.3743\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3531 - val_loss: 0.3634\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3506 - val_loss: 0.3615\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3503 - val_loss: 0.3601\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3487 - val_loss: 0.4854\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3504 - val_loss: 0.3497\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3467 - val_loss: 0.3559\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3458 - val_loss: 0.3516\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3444 - val_loss: 0.3559\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3429 - val_loss: 0.3498\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3414 - val_loss: 0.3528\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3413 - val_loss: 0.3490\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3398 - val_loss: 0.3599\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3398 - val_loss: 0.3501\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3382 - val_loss: 0.3465\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3382 - val_loss: 0.3640\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3371 - val_loss: 0.3478\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3365 - val_loss: 0.3422\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3362 - val_loss: 0.3459\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3355 - val_loss: 0.3507\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3342 - val_loss: 0.3421\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3336 - val_loss: 0.3400\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3319 - val_loss: 0.3399\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3325 - val_loss: 0.3701\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3319 - val_loss: 0.3401\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3306 - val_loss: 0.3395\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3298 - val_loss: 0.3429\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3291 - val_loss: 0.3444\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3297 - val_loss: 0.3396\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3284 - val_loss: 0.3552\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3271 - val_loss: 0.3332\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3275 - val_loss: 0.3421\n",
            "Epoch 54/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3275 - val_loss: 0.3365\n",
            "Epoch 55/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3257 - val_loss: 0.3851\n",
            "Epoch 56/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3247 - val_loss: 0.3689\n",
            "Epoch 57/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3258 - val_loss: 0.4224\n",
            "Epoch 58/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3249 - val_loss: 0.3329\n",
            "Epoch 59/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3242 - val_loss: 0.3368\n",
            "Epoch 60/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3239 - val_loss: 0.3794\n",
            "Epoch 61/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3229 - val_loss: 0.3306\n",
            "Epoch 62/100\n",
            "6604/6604 [==============================] - 0s 74us/sample - loss: 0.3214 - val_loss: 0.3440\n",
            "Epoch 63/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3224 - val_loss: 0.3288\n",
            "Epoch 64/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.3205 - val_loss: 0.3367\n",
            "Epoch 65/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3203 - val_loss: 0.3270\n",
            "Epoch 66/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3196 - val_loss: 0.3631\n",
            "Epoch 67/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3191 - val_loss: 0.3319\n",
            "Epoch 68/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3189 - val_loss: 0.3318\n",
            "Epoch 69/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3172 - val_loss: 0.3255\n",
            "Epoch 70/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3168 - val_loss: 0.3261\n",
            "Epoch 71/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3170 - val_loss: 0.3649\n",
            "Epoch 72/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3158 - val_loss: 0.3776\n",
            "Epoch 73/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3165 - val_loss: 0.3934\n",
            "Epoch 74/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3152 - val_loss: 0.3394\n",
            "Epoch 75/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3148 - val_loss: 0.3775\n",
            "Epoch 76/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3133 - val_loss: 0.3309\n",
            "Epoch 77/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3131 - val_loss: 0.4087\n",
            "Epoch 78/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3141 - val_loss: 0.3248\n",
            "Epoch 79/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3119 - val_loss: 0.3418\n",
            "Epoch 80/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3121 - val_loss: 0.3224\n",
            "Epoch 81/100\n",
            "6604/6604 [==============================] - 0s 75us/sample - loss: 0.3121 - val_loss: 0.3446\n",
            "Epoch 82/100\n",
            "6604/6604 [==============================] - 1s 80us/sample - loss: 0.3102 - val_loss: 0.3450\n",
            "Epoch 83/100\n",
            "6604/6604 [==============================] - 1s 77us/sample - loss: 0.3111 - val_loss: 0.3243\n",
            "Epoch 84/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3098 - val_loss: 0.3392\n",
            "Epoch 85/100\n",
            "6604/6604 [==============================] - 1s 76us/sample - loss: 0.3090 - val_loss: 0.3242\n",
            "Epoch 86/100\n",
            "6604/6604 [==============================] - 1s 78us/sample - loss: 0.3086 - val_loss: 0.3521\n",
            "Epoch 87/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3081 - val_loss: 0.3231\n",
            "Epoch 88/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3078 - val_loss: 0.4030\n",
            "Epoch 89/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3079 - val_loss: 0.3303\n",
            "Epoch 90/100\n",
            "6604/6604 [==============================] - 1s 79us/sample - loss: 0.3061 - val_loss: 0.3360\n",
            "3303/3303 [==============================] - 0s 29us/sample - loss: 0.3067\n",
            "[CV]  learning_rate=0.00397171378356561, n_hidden=1, n_neurons=26, total=  48.8s\n",
            "[CV] learning_rate=0.00397171378356561, n_hidden=1, n_neurons=26 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 292us/sample - loss: 1.2515 - val_loss: 2.1745\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.6404 - val_loss: 0.7689\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.5388 - val_loss: 0.6238\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.4818 - val_loss: 0.5093\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.4450 - val_loss: 0.5150\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.4221 - val_loss: 0.4936\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.4068 - val_loss: 0.4808\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3967 - val_loss: 0.5267\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3886 - val_loss: 0.5653\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3827 - val_loss: 0.4930\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3764 - val_loss: 0.5357\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3740 - val_loss: 0.5311\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3699 - val_loss: 0.4728\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3676 - val_loss: 0.4455\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3641 - val_loss: 0.4607\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3610 - val_loss: 0.4237\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3594 - val_loss: 0.4344\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3569 - val_loss: 0.4075\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3550 - val_loss: 0.3819\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3524 - val_loss: 0.3693\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3515 - val_loss: 0.3617\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3499 - val_loss: 0.3674\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 94us/sample - loss: 0.3481 - val_loss: 0.3665\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3474 - val_loss: 0.3763\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3468 - val_loss: 0.3638\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3449 - val_loss: 0.3645\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3442 - val_loss: 0.3741\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3429 - val_loss: 0.3528\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3425 - val_loss: 0.3682\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3416 - val_loss: 0.3627\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3403 - val_loss: 0.3535\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3409 - val_loss: 0.3474\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3385 - val_loss: 0.3523\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3385 - val_loss: 0.3581\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3367 - val_loss: 0.3530\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3375 - val_loss: 0.3535\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3356 - val_loss: 0.3450\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3352 - val_loss: 0.3451\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3343 - val_loss: 0.3487\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3341 - val_loss: 0.3572\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3338 - val_loss: 0.3674\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3331 - val_loss: 0.3469\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3327 - val_loss: 0.3474\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3309 - val_loss: 0.3669\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3302 - val_loss: 0.3466\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3302 - val_loss: 0.3450\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3292 - val_loss: 0.3478\n",
            "3302/3302 [==============================] - 0s 35us/sample - loss: 0.3401\n",
            "[CV]  learning_rate=0.00397171378356561, n_hidden=1, n_neurons=26, total=  27.3s\n",
            "[CV] learning_rate=0.00397171378356561, n_hidden=1, n_neurons=26 .....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 257us/sample - loss: 1.1585 - val_loss: 0.7139\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.5642 - val_loss: 0.6188\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.4905 - val_loss: 0.4738\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 99us/sample - loss: 0.4436 - val_loss: 0.5468\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.4140 - val_loss: 0.4662\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3931 - val_loss: 0.4322\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3796 - val_loss: 0.5566\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3705 - val_loss: 0.4944\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3638 - val_loss: 0.4049\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3572 - val_loss: 0.4729\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3539 - val_loss: 0.4578\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 94us/sample - loss: 0.3510 - val_loss: 0.4351\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3481 - val_loss: 0.5079\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3455 - val_loss: 0.4069\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3426 - val_loss: 0.6016\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3423 - val_loss: 0.3622\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3389 - val_loss: 0.3819\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3388 - val_loss: 0.3591\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3352 - val_loss: 0.5033\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3359 - val_loss: 0.3774\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3343 - val_loss: 0.3994\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3336 - val_loss: 0.3531\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3322 - val_loss: 0.3561\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3314 - val_loss: 0.3539\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3301 - val_loss: 0.4062\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3292 - val_loss: 0.3475\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3274 - val_loss: 0.4244\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3267 - val_loss: 0.3508\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3265 - val_loss: 0.3963\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3264 - val_loss: 0.3942\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3236 - val_loss: 0.3404\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3234 - val_loss: 0.4201\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 76us/sample - loss: 0.3223 - val_loss: 0.3652\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3219 - val_loss: 0.3617\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3199 - val_loss: 0.3373\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3187 - val_loss: 0.3785\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3192 - val_loss: 0.3476\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3180 - val_loss: 0.4350\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3170 - val_loss: 0.3840\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3160 - val_loss: 0.3652\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3155 - val_loss: 0.3343\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3138 - val_loss: 0.3358\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3128 - val_loss: 0.4013\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 79us/sample - loss: 0.3134 - val_loss: 0.3303\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3119 - val_loss: 0.3677\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3111 - val_loss: 0.3335\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 78us/sample - loss: 0.3105 - val_loss: 0.3403\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.3093 - val_loss: 0.3656\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3087 - val_loss: 0.3471\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3090 - val_loss: 0.3808\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3075 - val_loss: 0.3910\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 1s 80us/sample - loss: 0.3068 - val_loss: 0.3863\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 1s 77us/sample - loss: 0.3061 - val_loss: 0.4322\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3069 - val_loss: 0.3443\n",
            "3302/3302 [==============================] - 0s 28us/sample - loss: 0.3358\n",
            "[CV]  learning_rate=0.00397171378356561, n_hidden=1, n_neurons=26, total=  30.9s\n",
            "[CV] learning_rate=0.003661556254114653, n_hidden=2, n_neurons=72 ....\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 2s 313us/sample - loss: 1.3091 - val_loss: 1.2170\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 1s 97us/sample - loss: 0.5812 - val_loss: 0.5823\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.4899 - val_loss: 0.4650\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.4438 - val_loss: 0.4267\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.4168 - val_loss: 0.4155\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3991 - val_loss: 0.4045\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3861 - val_loss: 0.3926\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3772 - val_loss: 0.3899\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3688 - val_loss: 0.3914\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3622 - val_loss: 0.3763\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3571 - val_loss: 0.3619\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3537 - val_loss: 0.3662\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3484 - val_loss: 0.3594\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3458 - val_loss: 0.3637\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 1s 101us/sample - loss: 0.3410 - val_loss: 0.3621\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3379 - val_loss: 0.3475\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3354 - val_loss: 0.3670\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3328 - val_loss: 0.3693\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3312 - val_loss: 0.3521\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3279 - val_loss: 0.3452\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3252 - val_loss: 0.3436\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3259 - val_loss: 0.3427\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3217 - val_loss: 0.3365\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3210 - val_loss: 0.3363\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3194 - val_loss: 0.3391\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3170 - val_loss: 0.3491\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3160 - val_loss: 0.3317\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3147 - val_loss: 0.3278\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3122 - val_loss: 0.3335\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.3112 - val_loss: 0.3282\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3089 - val_loss: 0.3282\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3095 - val_loss: 0.3266\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3066 - val_loss: 0.3331\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3042 - val_loss: 0.3621\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.3046 - val_loss: 0.3248\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3024 - val_loss: 0.3327\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3013 - val_loss: 0.3284\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3004 - val_loss: 0.3257\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.2994 - val_loss: 0.3194\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2979 - val_loss: 0.3462\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2978 - val_loss: 0.3324\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2970 - val_loss: 0.3195\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.2951 - val_loss: 0.3247\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.2943 - val_loss: 0.3159\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.2940 - val_loss: 0.3488\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2935 - val_loss: 0.3165\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2914 - val_loss: 0.3166\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.2898 - val_loss: 0.3159\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.2898 - val_loss: 0.3195\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2878 - val_loss: 0.3242\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2875 - val_loss: 0.3200\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.2861 - val_loss: 0.3120\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2854 - val_loss: 0.3305\n",
            "Epoch 54/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2842 - val_loss: 0.3390\n",
            "Epoch 55/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.2836 - val_loss: 0.3186\n",
            "Epoch 56/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2829 - val_loss: 0.3117\n",
            "Epoch 57/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2815 - val_loss: 0.3284\n",
            "Epoch 58/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.2816 - val_loss: 0.3198\n",
            "Epoch 59/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.2805 - val_loss: 0.3185\n",
            "Epoch 60/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2799 - val_loss: 0.3084\n",
            "Epoch 61/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2799 - val_loss: 0.3082\n",
            "Epoch 62/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.2781 - val_loss: 0.3144\n",
            "Epoch 63/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2764 - val_loss: 0.3230\n",
            "Epoch 64/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2778 - val_loss: 0.3194\n",
            "Epoch 65/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2750 - val_loss: 0.3455\n",
            "Epoch 66/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2754 - val_loss: 0.3055\n",
            "Epoch 67/100\n",
            "6604/6604 [==============================] - 1s 82us/sample - loss: 0.2738 - val_loss: 0.3249\n",
            "Epoch 68/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2732 - val_loss: 0.3035\n",
            "Epoch 69/100\n",
            "6604/6604 [==============================] - 1s 81us/sample - loss: 0.2727 - val_loss: 0.3281\n",
            "Epoch 70/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.2711 - val_loss: 0.3498\n",
            "Epoch 71/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.2735 - val_loss: 0.3082\n",
            "Epoch 72/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2703 - val_loss: 0.3141\n",
            "Epoch 73/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2713 - val_loss: 0.3031\n",
            "Epoch 74/100\n",
            "6604/6604 [==============================] - 1s 83us/sample - loss: 0.2691 - val_loss: 0.3154\n",
            "Epoch 75/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2687 - val_loss: 0.3275\n",
            "Epoch 76/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2686 - val_loss: 0.3076\n",
            "Epoch 77/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.2685 - val_loss: 0.3161\n",
            "Epoch 78/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2670 - val_loss: 0.3007\n",
            "Epoch 79/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2665 - val_loss: 0.3098\n",
            "Epoch 80/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.2655 - val_loss: 0.3111\n",
            "Epoch 81/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2643 - val_loss: 0.3083\n",
            "Epoch 82/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.2636 - val_loss: 0.3124\n",
            "Epoch 83/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.2640 - val_loss: 0.3023\n",
            "Epoch 84/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2633 - val_loss: 0.3502\n",
            "Epoch 85/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2629 - val_loss: 0.3056\n",
            "Epoch 86/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.2630 - val_loss: 0.3159\n",
            "Epoch 87/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.2605 - val_loss: 0.3056\n",
            "Epoch 88/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.2608 - val_loss: 0.3145\n",
            "3303/3303 [==============================] - 0s 30us/sample - loss: 0.2870\n",
            "[CV]  learning_rate=0.003661556254114653, n_hidden=2, n_neurons=72, total=  52.1s\n",
            "[CV] learning_rate=0.003661556254114653, n_hidden=2, n_neurons=72 ....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 312us/sample - loss: 1.2838 - val_loss: 0.8980\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 81us/sample - loss: 0.5547 - val_loss: 0.4956\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.4709 - val_loss: 0.4595\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.4272 - val_loss: 0.5590\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.4033 - val_loss: 0.7057\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3871 - val_loss: 0.7324\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3752 - val_loss: 0.7117\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3678 - val_loss: 0.5753\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3598 - val_loss: 0.5503\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 82us/sample - loss: 0.3533 - val_loss: 0.4838\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3487 - val_loss: 0.4324\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3444 - val_loss: 0.3983\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3408 - val_loss: 0.3971\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3371 - val_loss: 0.4178\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3345 - val_loss: 0.3861\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3326 - val_loss: 0.3685\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3302 - val_loss: 0.3506\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3279 - val_loss: 0.3589\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3253 - val_loss: 0.3949\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3235 - val_loss: 0.3336\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3223 - val_loss: 0.3597\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3211 - val_loss: 0.3648\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3191 - val_loss: 0.3742\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3179 - val_loss: 0.3510\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3156 - val_loss: 0.3411\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3146 - val_loss: 0.3617\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3125 - val_loss: 0.3618\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3123 - val_loss: 0.3743\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3109 - val_loss: 0.3429\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3090 - val_loss: 0.3474\n",
            "3302/3302 [==============================] - 0s 35us/sample - loss: 0.3379\n",
            "[CV]  learning_rate=0.003661556254114653, n_hidden=2, n_neurons=72, total=  18.9s\n",
            "[CV] learning_rate=0.003661556254114653, n_hidden=2, n_neurons=72 ....\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 314us/sample - loss: 1.1983 - val_loss: 0.7533\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.5085 - val_loss: 0.4909\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.4468 - val_loss: 0.4385\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.4131 - val_loss: 0.4397\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3947 - val_loss: 0.4027\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3778 - val_loss: 0.3855\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3679 - val_loss: 0.3857\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3607 - val_loss: 0.6635\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3564 - val_loss: 0.4119\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3501 - val_loss: 0.6304\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3460 - val_loss: 0.3803\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3407 - val_loss: 0.5012\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 83us/sample - loss: 0.3394 - val_loss: 0.3977\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3360 - val_loss: 0.4084\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3335 - val_loss: 0.3514\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3318 - val_loss: 0.3499\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3296 - val_loss: 0.3544\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3288 - val_loss: 0.3980\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3256 - val_loss: 0.7729\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3260 - val_loss: 0.3509\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3232 - val_loss: 0.3502\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3215 - val_loss: 0.3937\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3209 - val_loss: 0.3658\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3190 - val_loss: 0.3697\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3168 - val_loss: 0.3379\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3169 - val_loss: 0.4130\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3145 - val_loss: 0.3424\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3138 - val_loss: 0.3608\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3123 - val_loss: 0.5162\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3113 - val_loss: 0.3321\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 84us/sample - loss: 0.3100 - val_loss: 0.3579\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3085 - val_loss: 0.3309\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3081 - val_loss: 0.4516\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3075 - val_loss: 0.3412\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3060 - val_loss: 0.3902\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3047 - val_loss: 0.3376\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3052 - val_loss: 0.3784\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3025 - val_loss: 0.3268\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3019 - val_loss: 0.4427\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3012 - val_loss: 0.3365\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2999 - val_loss: 0.3493\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.2978 - val_loss: 0.3901\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.2975 - val_loss: 0.3400\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.2962 - val_loss: 0.3275\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.2968 - val_loss: 0.3630\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.2960 - val_loss: 0.3278\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2928 - val_loss: 0.3272\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.2932 - val_loss: 0.4075\n",
            "3302/3302 [==============================] - 0s 36us/sample - loss: 0.3446\n",
            "[CV]  learning_rate=0.003661556254114653, n_hidden=2, n_neurons=72, total=  29.6s\n",
            "[CV] learning_rate=0.0009937461361172427, n_hidden=3, n_neurons=42 ...\n",
            "Train on 6604 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6604/6604 [==============================] - 2s 277us/sample - loss: 2.5756 - val_loss: 11.5495\n",
            "Epoch 2/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 1.1289 - val_loss: 1.9069\n",
            "Epoch 3/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.8496 - val_loss: 0.8472\n",
            "Epoch 4/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.7630 - val_loss: 0.7324\n",
            "Epoch 5/100\n",
            "6604/6604 [==============================] - 1s 96us/sample - loss: 0.7124 - val_loss: 0.6911\n",
            "Epoch 6/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.6761 - val_loss: 0.6852\n",
            "Epoch 7/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.6448 - val_loss: 0.6476\n",
            "Epoch 8/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.6193 - val_loss: 0.6213\n",
            "Epoch 9/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.5957 - val_loss: 0.5974\n",
            "Epoch 10/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.5741 - val_loss: 0.5887\n",
            "Epoch 11/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.5546 - val_loss: 0.5573\n",
            "Epoch 12/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.5369 - val_loss: 0.5342\n",
            "Epoch 13/100\n",
            "6604/6604 [==============================] - 1s 98us/sample - loss: 0.5209 - val_loss: 0.5157\n",
            "Epoch 14/100\n",
            "6604/6604 [==============================] - 1s 96us/sample - loss: 0.5059 - val_loss: 0.4945\n",
            "Epoch 15/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.4924 - val_loss: 0.4834\n",
            "Epoch 16/100\n",
            "6604/6604 [==============================] - 1s 95us/sample - loss: 0.4801 - val_loss: 0.4711\n",
            "Epoch 17/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.4687 - val_loss: 0.4686\n",
            "Epoch 18/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.4583 - val_loss: 0.4514\n",
            "Epoch 19/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.4486 - val_loss: 0.4418\n",
            "Epoch 20/100\n",
            "6604/6604 [==============================] - 1s 92us/sample - loss: 0.4408 - val_loss: 0.4343\n",
            "Epoch 21/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.4327 - val_loss: 0.4284\n",
            "Epoch 22/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.4259 - val_loss: 0.4220\n",
            "Epoch 23/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.4196 - val_loss: 0.4174\n",
            "Epoch 24/100\n",
            "6604/6604 [==============================] - 1s 93us/sample - loss: 0.4137 - val_loss: 0.4172\n",
            "Epoch 25/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.4089 - val_loss: 0.4117\n",
            "Epoch 26/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.4038 - val_loss: 0.4099\n",
            "Epoch 27/100\n",
            "6604/6604 [==============================] - 1s 94us/sample - loss: 0.3995 - val_loss: 0.4052\n",
            "Epoch 28/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3957 - val_loss: 0.4007\n",
            "Epoch 29/100\n",
            "6604/6604 [==============================] - 1s 93us/sample - loss: 0.3918 - val_loss: 0.3999\n",
            "Epoch 30/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3886 - val_loss: 0.3925\n",
            "Epoch 31/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3856 - val_loss: 0.3911\n",
            "Epoch 32/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3833 - val_loss: 0.3896\n",
            "Epoch 33/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3803 - val_loss: 0.3832\n",
            "Epoch 34/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3775 - val_loss: 0.3825\n",
            "Epoch 35/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3756 - val_loss: 0.3806\n",
            "Epoch 36/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3732 - val_loss: 0.3806\n",
            "Epoch 37/100\n",
            "6604/6604 [==============================] - 1s 92us/sample - loss: 0.3714 - val_loss: 0.3815\n",
            "Epoch 38/100\n",
            "6604/6604 [==============================] - 1s 92us/sample - loss: 0.3693 - val_loss: 0.3740\n",
            "Epoch 39/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3675 - val_loss: 0.3736\n",
            "Epoch 40/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3656 - val_loss: 0.3707\n",
            "Epoch 41/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3641 - val_loss: 0.3722\n",
            "Epoch 42/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3625 - val_loss: 0.3653\n",
            "Epoch 43/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3609 - val_loss: 0.3645\n",
            "Epoch 44/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3592 - val_loss: 0.3669\n",
            "Epoch 45/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3579 - val_loss: 0.3646\n",
            "Epoch 46/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3569 - val_loss: 0.3624\n",
            "Epoch 47/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3551 - val_loss: 0.3615\n",
            "Epoch 48/100\n",
            "6604/6604 [==============================] - 1s 93us/sample - loss: 0.3541 - val_loss: 0.3584\n",
            "Epoch 49/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3531 - val_loss: 0.3589\n",
            "Epoch 50/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3514 - val_loss: 0.3583\n",
            "Epoch 51/100\n",
            "6604/6604 [==============================] - 1s 90us/sample - loss: 0.3511 - val_loss: 0.3558\n",
            "Epoch 52/100\n",
            "6604/6604 [==============================] - 1s 95us/sample - loss: 0.3498 - val_loss: 0.3562\n",
            "Epoch 53/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3481 - val_loss: 0.3572\n",
            "Epoch 54/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3479 - val_loss: 0.3548\n",
            "Epoch 55/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3469 - val_loss: 0.3551\n",
            "Epoch 56/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3462 - val_loss: 0.3524\n",
            "Epoch 57/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3451 - val_loss: 0.3508\n",
            "Epoch 58/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3442 - val_loss: 0.3513\n",
            "Epoch 59/100\n",
            "6604/6604 [==============================] - 1s 93us/sample - loss: 0.3434 - val_loss: 0.3519\n",
            "Epoch 60/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3426 - val_loss: 0.3499\n",
            "Epoch 61/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.3415 - val_loss: 0.3496\n",
            "Epoch 62/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3412 - val_loss: 0.3487\n",
            "Epoch 63/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3401 - val_loss: 0.3494\n",
            "Epoch 64/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3396 - val_loss: 0.3476\n",
            "Epoch 65/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3389 - val_loss: 0.3485\n",
            "Epoch 66/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3382 - val_loss: 0.3480\n",
            "Epoch 67/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3375 - val_loss: 0.3460\n",
            "Epoch 68/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3369 - val_loss: 0.3483\n",
            "Epoch 69/100\n",
            "6604/6604 [==============================] - 1s 98us/sample - loss: 0.3366 - val_loss: 0.3457\n",
            "Epoch 70/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3357 - val_loss: 0.3456\n",
            "Epoch 71/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3346 - val_loss: 0.3455\n",
            "Epoch 72/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3346 - val_loss: 0.3445\n",
            "Epoch 73/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3341 - val_loss: 0.3444\n",
            "Epoch 74/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3330 - val_loss: 0.3448\n",
            "Epoch 75/100\n",
            "6604/6604 [==============================] - 1s 92us/sample - loss: 0.3327 - val_loss: 0.3442\n",
            "Epoch 76/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3325 - val_loss: 0.3444\n",
            "Epoch 77/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.3320 - val_loss: 0.3426\n",
            "Epoch 78/100\n",
            "6604/6604 [==============================] - 1s 84us/sample - loss: 0.3309 - val_loss: 0.3445\n",
            "Epoch 79/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3309 - val_loss: 0.3414\n",
            "Epoch 80/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3303 - val_loss: 0.3423\n",
            "Epoch 81/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3296 - val_loss: 0.3420\n",
            "Epoch 82/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3294 - val_loss: 0.3406\n",
            "Epoch 83/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3288 - val_loss: 0.3424\n",
            "Epoch 84/100\n",
            "6604/6604 [==============================] - 1s 92us/sample - loss: 0.3288 - val_loss: 0.3417\n",
            "Epoch 85/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3282 - val_loss: 0.3401\n",
            "Epoch 86/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3274 - val_loss: 0.3410\n",
            "Epoch 87/100\n",
            "6604/6604 [==============================] - 1s 85us/sample - loss: 0.3270 - val_loss: 0.3403\n",
            "Epoch 88/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3269 - val_loss: 0.3390\n",
            "Epoch 89/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3265 - val_loss: 0.3388\n",
            "Epoch 90/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3257 - val_loss: 0.3399\n",
            "Epoch 91/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3251 - val_loss: 0.3409\n",
            "Epoch 92/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3251 - val_loss: 0.3374\n",
            "Epoch 93/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3245 - val_loss: 0.3373\n",
            "Epoch 94/100\n",
            "6604/6604 [==============================] - 1s 89us/sample - loss: 0.3239 - val_loss: 0.3376\n",
            "Epoch 95/100\n",
            "6604/6604 [==============================] - 1s 97us/sample - loss: 0.3233 - val_loss: 0.3363\n",
            "Epoch 96/100\n",
            "6604/6604 [==============================] - 1s 91us/sample - loss: 0.3233 - val_loss: 0.3381\n",
            "Epoch 97/100\n",
            "6604/6604 [==============================] - 1s 88us/sample - loss: 0.3231 - val_loss: 0.3371\n",
            "Epoch 98/100\n",
            "6604/6604 [==============================] - 1s 87us/sample - loss: 0.3228 - val_loss: 0.3365\n",
            "Epoch 99/100\n",
            "6604/6604 [==============================] - 1s 86us/sample - loss: 0.3219 - val_loss: 0.3364\n",
            "Epoch 100/100\n",
            "6604/6604 [==============================] - 1s 92us/sample - loss: 0.3217 - val_loss: 0.3361\n",
            "3303/3303 [==============================] - 0s 37us/sample - loss: 0.3210\n",
            "[CV]  learning_rate=0.0009937461361172427, n_hidden=3, n_neurons=42, total= 1.0min\n",
            "[CV] learning_rate=0.0009937461361172427, n_hidden=3, n_neurons=42 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 303us/sample - loss: 2.3391 - val_loss: 8.9131\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.8118 - val_loss: 4.7924\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.6980 - val_loss: 2.8646\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.6384 - val_loss: 1.9443\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.5944 - val_loss: 1.4341\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.5579 - val_loss: 1.0984\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.5275 - val_loss: 0.9107\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.5026 - val_loss: 0.7482\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.4814 - val_loss: 0.6823\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.4646 - val_loss: 0.6145\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.4503 - val_loss: 0.5763\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.4382 - val_loss: 0.5446\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.4283 - val_loss: 0.5251\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.4203 - val_loss: 0.5069\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.4123 - val_loss: 0.4953\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.4058 - val_loss: 0.4855\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3996 - val_loss: 0.4779\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3941 - val_loss: 0.4742\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3894 - val_loss: 0.4733\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3856 - val_loss: 0.4708\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3810 - val_loss: 0.4669\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3773 - val_loss: 0.4696\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3743 - val_loss: 0.4637\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3711 - val_loss: 0.4710\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3681 - val_loss: 0.4672\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3659 - val_loss: 0.4714\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3639 - val_loss: 0.4711\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3615 - val_loss: 0.4686\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3598 - val_loss: 0.4684\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 85us/sample - loss: 0.3581 - val_loss: 0.4746\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3565 - val_loss: 0.4791\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3548 - val_loss: 0.4731\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3534 - val_loss: 0.4848\n",
            "3302/3302 [==============================] - 0s 34us/sample - loss: 0.3598\n",
            "[CV]  learning_rate=0.0009937461361172427, n_hidden=3, n_neurons=42, total=  21.0s\n",
            "[CV] learning_rate=0.0009937461361172427, n_hidden=3, n_neurons=42 ...\n",
            "Train on 6605 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "6605/6605 [==============================] - 2s 330us/sample - loss: 2.4917 - val_loss: 1.9190\n",
            "Epoch 2/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.8314 - val_loss: 0.9591\n",
            "Epoch 3/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.6464 - val_loss: 0.6730\n",
            "Epoch 4/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.5764 - val_loss: 0.5825\n",
            "Epoch 5/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.5414 - val_loss: 0.5437\n",
            "Epoch 6/100\n",
            "6605/6605 [==============================] - 1s 97us/sample - loss: 0.5158 - val_loss: 0.5162\n",
            "Epoch 7/100\n",
            "6605/6605 [==============================] - 1s 95us/sample - loss: 0.4944 - val_loss: 0.4966\n",
            "Epoch 8/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.4757 - val_loss: 0.4811\n",
            "Epoch 9/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.4586 - val_loss: 0.4651\n",
            "Epoch 10/100\n",
            "6605/6605 [==============================] - 1s 94us/sample - loss: 0.4438 - val_loss: 0.4561\n",
            "Epoch 11/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.4312 - val_loss: 0.4411\n",
            "Epoch 12/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.4191 - val_loss: 0.4317\n",
            "Epoch 13/100\n",
            "6605/6605 [==============================] - 1s 97us/sample - loss: 0.4087 - val_loss: 0.4222\n",
            "Epoch 14/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3998 - val_loss: 0.4172\n",
            "Epoch 15/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3915 - val_loss: 0.4117\n",
            "Epoch 16/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3853 - val_loss: 0.4057\n",
            "Epoch 17/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3791 - val_loss: 0.4007\n",
            "Epoch 18/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3739 - val_loss: 0.4000\n",
            "Epoch 19/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3699 - val_loss: 0.3945\n",
            "Epoch 20/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3661 - val_loss: 0.3904\n",
            "Epoch 21/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3623 - val_loss: 0.3886\n",
            "Epoch 22/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3592 - val_loss: 0.3849\n",
            "Epoch 23/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3565 - val_loss: 0.3823\n",
            "Epoch 24/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3544 - val_loss: 0.3800\n",
            "Epoch 25/100\n",
            "6605/6605 [==============================] - 1s 95us/sample - loss: 0.3516 - val_loss: 0.3784\n",
            "Epoch 26/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3492 - val_loss: 0.3759\n",
            "Epoch 27/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3475 - val_loss: 0.3732\n",
            "Epoch 28/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3461 - val_loss: 0.3687\n",
            "Epoch 29/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3438 - val_loss: 0.3706\n",
            "Epoch 30/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3428 - val_loss: 0.3690\n",
            "Epoch 31/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3413 - val_loss: 0.3649\n",
            "Epoch 32/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3398 - val_loss: 0.3620\n",
            "Epoch 33/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3387 - val_loss: 0.3616\n",
            "Epoch 34/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3375 - val_loss: 0.3605\n",
            "Epoch 35/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3360 - val_loss: 0.3604\n",
            "Epoch 36/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3351 - val_loss: 0.3582\n",
            "Epoch 37/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3340 - val_loss: 0.3586\n",
            "Epoch 38/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3325 - val_loss: 0.3583\n",
            "Epoch 39/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3318 - val_loss: 0.3572\n",
            "Epoch 40/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3311 - val_loss: 0.3562\n",
            "Epoch 41/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3300 - val_loss: 0.3545\n",
            "Epoch 42/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3293 - val_loss: 0.3533\n",
            "Epoch 43/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3282 - val_loss: 0.3517\n",
            "Epoch 44/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3275 - val_loss: 0.3503\n",
            "Epoch 45/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3268 - val_loss: 0.3524\n",
            "Epoch 46/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3260 - val_loss: 0.3492\n",
            "Epoch 47/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3250 - val_loss: 0.3497\n",
            "Epoch 48/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3245 - val_loss: 0.3487\n",
            "Epoch 49/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3239 - val_loss: 0.3480\n",
            "Epoch 50/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3230 - val_loss: 0.3485\n",
            "Epoch 51/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3222 - val_loss: 0.3456\n",
            "Epoch 52/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3215 - val_loss: 0.3452\n",
            "Epoch 53/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3212 - val_loss: 0.3448\n",
            "Epoch 54/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3206 - val_loss: 0.3455\n",
            "Epoch 55/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3196 - val_loss: 0.3442\n",
            "Epoch 56/100\n",
            "6605/6605 [==============================] - 1s 94us/sample - loss: 0.3191 - val_loss: 0.3435\n",
            "Epoch 57/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3184 - val_loss: 0.3428\n",
            "Epoch 58/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3182 - val_loss: 0.3417\n",
            "Epoch 59/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3170 - val_loss: 0.3428\n",
            "Epoch 60/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3165 - val_loss: 0.3428\n",
            "Epoch 61/100\n",
            "6605/6605 [==============================] - 1s 95us/sample - loss: 0.3163 - val_loss: 0.3395\n",
            "Epoch 62/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3157 - val_loss: 0.3391\n",
            "Epoch 63/100\n",
            "6605/6605 [==============================] - 1s 95us/sample - loss: 0.3150 - val_loss: 0.3403\n",
            "Epoch 64/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3145 - val_loss: 0.3402\n",
            "Epoch 65/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3142 - val_loss: 0.3378\n",
            "Epoch 66/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3133 - val_loss: 0.3376\n",
            "Epoch 67/100\n",
            "6605/6605 [==============================] - 1s 86us/sample - loss: 0.3127 - val_loss: 0.3394\n",
            "Epoch 68/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3118 - val_loss: 0.3373\n",
            "Epoch 69/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3117 - val_loss: 0.3356\n",
            "Epoch 70/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3116 - val_loss: 0.3434\n",
            "Epoch 71/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3108 - val_loss: 0.3350\n",
            "Epoch 72/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3102 - val_loss: 0.3342\n",
            "Epoch 73/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3096 - val_loss: 0.3352\n",
            "Epoch 74/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3093 - val_loss: 0.3372\n",
            "Epoch 75/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3086 - val_loss: 0.3337\n",
            "Epoch 76/100\n",
            "6605/6605 [==============================] - 1s 92us/sample - loss: 0.3076 - val_loss: 0.3334\n",
            "Epoch 77/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3074 - val_loss: 0.3354\n",
            "Epoch 78/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3074 - val_loss: 0.3356\n",
            "Epoch 79/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3069 - val_loss: 0.3316\n",
            "Epoch 80/100\n",
            "6605/6605 [==============================] - 1s 96us/sample - loss: 0.3066 - val_loss: 0.3316\n",
            "Epoch 81/100\n",
            "6605/6605 [==============================] - 1s 87us/sample - loss: 0.3061 - val_loss: 0.3315\n",
            "Epoch 82/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3051 - val_loss: 0.3315\n",
            "Epoch 83/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3052 - val_loss: 0.3310\n",
            "Epoch 84/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3050 - val_loss: 0.3308\n",
            "Epoch 85/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3043 - val_loss: 0.3298\n",
            "Epoch 86/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3040 - val_loss: 0.3330\n",
            "Epoch 87/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.3033 - val_loss: 0.3299\n",
            "Epoch 88/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3029 - val_loss: 0.3292\n",
            "Epoch 89/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3027 - val_loss: 0.3295\n",
            "Epoch 90/100\n",
            "6605/6605 [==============================] - 1s 93us/sample - loss: 0.3025 - val_loss: 0.3285\n",
            "Epoch 91/100\n",
            "6605/6605 [==============================] - 1s 94us/sample - loss: 0.3019 - val_loss: 0.3292\n",
            "Epoch 92/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3015 - val_loss: 0.3277\n",
            "Epoch 93/100\n",
            "6605/6605 [==============================] - 1s 89us/sample - loss: 0.3009 - val_loss: 0.3275\n",
            "Epoch 94/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3010 - val_loss: 0.3283\n",
            "Epoch 95/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.3005 - val_loss: 0.3318\n",
            "Epoch 96/100\n",
            "6605/6605 [==============================] - 1s 88us/sample - loss: 0.3000 - val_loss: 0.3270\n",
            "Epoch 97/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.2995 - val_loss: 0.3282\n",
            "Epoch 98/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.2993 - val_loss: 0.3274\n",
            "Epoch 99/100\n",
            "6605/6605 [==============================] - 1s 90us/sample - loss: 0.2985 - val_loss: 0.3302\n",
            "Epoch 100/100\n",
            "6605/6605 [==============================] - 1s 91us/sample - loss: 0.2981 - val_loss: 0.3310\n",
            "3302/3302 [==============================] - 0s 35us/sample - loss: 0.3474\n",
            "[CV]  learning_rate=0.0009937461361172427, n_hidden=3, n_neurons=42, total= 1.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 14.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 9907 samples, validate on 6605 samples\n",
            "Epoch 1/100\n",
            "9907/9907 [==============================] - 3s 270us/sample - loss: 0.5591 - val_loss: 0.4376\n",
            "Epoch 2/100\n",
            "9907/9907 [==============================] - 1s 80us/sample - loss: 0.3864 - val_loss: 2.2227\n",
            "Epoch 3/100\n",
            "9907/9907 [==============================] - 1s 79us/sample - loss: 0.3664 - val_loss: 0.6718\n",
            "Epoch 4/100\n",
            "9907/9907 [==============================] - 1s 81us/sample - loss: 0.3409 - val_loss: 0.3478\n",
            "Epoch 5/100\n",
            "9907/9907 [==============================] - 1s 80us/sample - loss: 0.3344 - val_loss: 0.3538\n",
            "Epoch 6/100\n",
            "9907/9907 [==============================] - 1s 75us/sample - loss: 0.3288 - val_loss: 0.3558\n",
            "Epoch 7/100\n",
            "9907/9907 [==============================] - 1s 77us/sample - loss: 0.3242 - val_loss: 0.4187\n",
            "Epoch 8/100\n",
            "9907/9907 [==============================] - 1s 81us/sample - loss: 0.3211 - val_loss: 0.3320\n",
            "Epoch 9/100\n",
            "9907/9907 [==============================] - 1s 77us/sample - loss: 0.3138 - val_loss: 0.3290\n",
            "Epoch 10/100\n",
            "9907/9907 [==============================] - 1s 80us/sample - loss: 0.3079 - val_loss: 0.3362\n",
            "Epoch 11/100\n",
            "9907/9907 [==============================] - 1s 78us/sample - loss: 0.3065 - val_loss: 0.3165\n",
            "Epoch 12/100\n",
            "9907/9907 [==============================] - 1s 78us/sample - loss: 0.3006 - val_loss: 0.3276\n",
            "Epoch 13/100\n",
            "9907/9907 [==============================] - 1s 81us/sample - loss: 0.2989 - val_loss: 0.5655\n",
            "Epoch 14/100\n",
            "9907/9907 [==============================] - 1s 81us/sample - loss: 0.2949 - val_loss: 0.3369\n",
            "Epoch 15/100\n",
            "9907/9907 [==============================] - 1s 80us/sample - loss: 0.2903 - val_loss: 0.7415\n",
            "Epoch 16/100\n",
            "9907/9907 [==============================] - 1s 77us/sample - loss: 0.2952 - val_loss: 0.3936\n",
            "Epoch 17/100\n",
            "9907/9907 [==============================] - 1s 80us/sample - loss: 0.2871 - val_loss: 0.3579\n",
            "Epoch 18/100\n",
            "9907/9907 [==============================] - 1s 86us/sample - loss: 0.2840 - val_loss: 0.3231\n",
            "Epoch 19/100\n",
            "9907/9907 [==============================] - 1s 86us/sample - loss: 0.2829 - val_loss: 0.3510\n",
            "Epoch 20/100\n",
            "9907/9907 [==============================] - 1s 88us/sample - loss: 0.2792 - val_loss: 0.3467\n",
            "Epoch 21/100\n",
            "9907/9907 [==============================] - 1s 90us/sample - loss: 0.2769 - val_loss: 0.3206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f26c7ae0128>,\n",
              "                   iid='warn', n_iter=10, n_jobs=None,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f26c7a4c5c0>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([10, 11, 12, 13, 14, 15, 16,...18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
              "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
              "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
              "       61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n",
              "       78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94,\n",
              "       95, 96, 97, 98, 99])},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMH5WWeeYyxS",
        "colab_type": "text"
      },
      "source": [
        "Best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJsZ2ICNmVy3",
        "colab_type": "code",
        "outputId": "d834e268-04d3-42d4-c9d2-aa3383b6f7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.01543734951823562, 'n_hidden': 2, 'n_neurons': 89}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s6A478vY3-0",
        "colab_type": "text"
      },
      "source": [
        "Best score with best parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka-4X2Jom0dN",
        "colab_type": "code",
        "outputId": "5f577a06-cbf1-4fa4-d4ac-086892161578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.2998488925873445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZe2mKmdY8Kw",
        "colab_type": "text"
      },
      "source": [
        "Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlNrnl0hm3aa",
        "colab_type": "code",
        "outputId": "97a7c7d5-dbf3-4ebc-b0cb-bb538cb19e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "rnd_search_cv.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x7f26c6d89908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jljuZAsVm7_v",
        "colab_type": "text"
      },
      "source": [
        "Evaluate best fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eotIx3em35x",
        "colab_type": "code",
        "outputId": "a409bd7d-c805-4d04-db48-fedd79b669e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rnd_search_cv.score(X_test_scaled, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4128/4128 [==============================] - 0s 40us/sample - loss: 0.4248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.4247736125722412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAwlx6_oZDkM",
        "colab_type": "text"
      },
      "source": [
        "Evaluate best Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adLU3HHnEoy",
        "colab_type": "code",
        "outputId": "9a5e0539-caaa-4edf-9d00-9f111c3b07a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model\n",
        "\n",
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4128/4128 [==============================] - 0s 35us/sample - loss: 0.4248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4247736125722412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSmbf6EBZLJ4",
        "colab_type": "text"
      },
      "source": [
        "Save model to G drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVDFcz4hnH-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"cal_housing_reg_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X8-DyNQnSru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}